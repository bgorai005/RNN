{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3485474,"sourceType":"datasetVersion","datasetId":2097669}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Necssary libary","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport pandas as pd\nimport unicodedata\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport wandb\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:36:36.184282Z","iopub.execute_input":"2025-05-19T08:36:36.184457Z","iopub.status.idle":"2025-05-19T08:36:36.188899Z","shell.execute_reply.started":"2025-05-19T08:36:36.184441Z","shell.execute_reply":"2025-05-19T08:36:36.188092Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_path = '/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv'\nvalid_path = '/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv'\ntest_path  = '/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:36:36.190195Z","iopub.execute_input":"2025-05-19T08:36:36.190506Z","iopub.status.idle":"2025-05-19T08:36:36.211240Z","shell.execute_reply.started":"2025-05-19T08:36:36.190478Z","shell.execute_reply":"2025-05-19T08:36:36.210365Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"594642013968a68e466138e783dcece6765c43b9\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:37:16.012378Z","iopub.execute_input":"2025-05-19T08:37:16.012911Z","iopub.status.idle":"2025-05-19T08:37:22.426143Z","shell.execute_reply.started":"2025-05-19T08:37:16.012880Z","shell.execute_reply":"2025-05-19T08:37:22.425554Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbgorai005\u001b[0m (\u001b[33mbgorai005-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Encoder, decoder and seq2seq model class","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_size, embedding_dim, hidden_size, num_layers=1, cell_type='LSTM', dropout=0.2):\n        super(Encoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(input_size, embedding_dim)\n        rnn_class = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[cell_type]\n        self.rnn = rnn_class(\n            input_size=embedding_dim,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_seq):\n        embedded = self.dropout(self.embedding(input_seq))\n        batch_size = input_seq.size(0)\n        device = input_seq.device\n        if self.cell_type == 'LSTM':\n            hidden = (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n                     torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n        else:\n            hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        output, hidden = self.rnn(embedded, hidden)\n        return output, hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embedding_dim, hidden_size, num_layers=1, cell_type='LSTM', dropout=0.2):\n        super(Decoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(output_size, embedding_dim)\n        rnn_class = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[cell_type]\n        self.rnn = rnn_class(\n            input_size=embedding_dim,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        self.out = nn.Linear(hidden_size, output_size)\n        self.dropout = nn.Dropout(dropout)\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n    def forward(self, input_char, hidden):\n        embedded = self.dropout(self.embedding(input_char))\n        output, hidden = self.rnn(embedded, hidden)\n        output = self.softmax(self.out(output.squeeze(1)))\n        return output, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_forcing_ratio=0.5):\n        batch_size = source.size(0)\n        target_len = target.size(1)\n        target_vocab_size = self.decoder.embedding.num_embeddings\n        device = source.device\n        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(device)\n        _, hidden = self.encoder(source)\n        if self.encoder.num_layers != self.decoder.num_layers:\n            if self.encoder.cell_type == 'LSTM':\n                h_n, c_n = hidden\n                if self.decoder.num_layers > self.encoder.num_layers:\n                    extra_layers = self.decoder.num_layers - self.encoder.num_layers\n                    extra_h = torch.zeros(extra_layers, batch_size, self.decoder.hidden_size).to(device)\n                    extra_c = torch.zeros(extra_layers, batch_size, self.decoder.hidden_size).to(device)\n                    h_n = torch.cat([h_n, extra_h], dim=0)\n                    c_n = torch.cat([c_n, extra_c], dim=0)\n                else:\n                    h_n = h_n[:self.decoder.num_layers]\n                    c_n = c_n[:self.decoder.num_layers]\n                hidden = (h_n, c_n)\n            else:\n                if self.decoder.num_layers > self.encoder.num_layers:\n                    extra_layers = self.decoder.num_layers - self.encoder.num_layers\n                    extra_h = torch.zeros(extra_layers, batch_size, self.decoder.hidden_size).to(device)\n                    hidden = torch.cat([hidden, extra_h], dim=0)\n                else:\n                    hidden = hidden[:self.decoder.num_layers]\n        decoder_input = target[:, 0].unsqueeze(1)\n        for t in range(1, target_len):\n            output, hidden = self.decoder(decoder_input, hidden)\n            outputs[:, t] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            decoder_input = target[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n        return outputs\n\n    def predict(self, src, max_len=30, beam_size=3):\n        self.eval()\n        batch_size = src.size(0)\n        device = src.device\n        _, hidden = self.encoder(src)\n        outputs = []\n        for i in range(batch_size):\n            if self.encoder.cell_type == 'LSTM':\n                h = hidden[0][:, i:i+1].contiguous()\n                c = hidden[1][:, i:i+1].contiguous()\n                hidden_state = (h, c)\n            else:\n                hidden_state = hidden[:, i:i+1].contiguous()\n            if self.encoder.num_layers != self.decoder.num_layers:\n                if self.encoder.cell_type == 'LSTM':\n                    h_n, c_n = hidden_state\n                    if self.decoder.num_layers > self.encoder.num_layers:\n                        extra_layers = self.decoder.num_layers - self.encoder.num_layers\n                        extra_h = torch.zeros(extra_layers, 1, self.decoder.hidden_size).to(device)\n                        extra_c = torch.zeros(extra_layers, 1, self.decoder.hidden_size).to(device)\n                        h_n = torch.cat([h_n, extra_h], dim=0)\n                        c_n = torch.cat([c_n, extra_c], dim=0)\n                    else:\n                        h_n = h_n[:self.decoder.num_layers]\n                        c_n = c_n[:self.decoder.num_layers]\n                    hidden_state = (h_n, c_n)\n                else:\n                    if self.decoder.num_layers > self.encoder.num_layers:\n                        extra_layers = self.decoder.num_layers - self.encoder.num_layers\n                        extra_h = torch.zeros(extra_layers, 1, self.decoder.hidden_size).to(device)\n                        hidden_state = torch.cat([hidden_state, extra_h], dim=0)\n                    else:\n                        hidden_state = hidden_state[:self.decoder.num_layers]\n            beams = [(torch.tensor([1], device=device), 0.0, hidden_state)]  # [sequence, score, hidden]\n            for _ in range(max_len):\n                new_beams = []\n                for seq, score, h in beams:\n                    input_char = seq[-1].unsqueeze(0).unsqueeze(0)\n                    output, h_new = self.decoder(input_char, h)\n                    probs = torch.log_softmax(output, dim=-1).squeeze(0)\n                    topk = torch.topk(probs, beam_size)\n                    for idx, prob in zip(topk.indices, topk.values):\n                        new_seq = torch.cat([seq, idx.unsqueeze(0)])\n                        new_score = score + prob.item()\n                        if self.decoder.cell_type == 'LSTM':\n                            h_new = (h_new[0].contiguous(), h_new[1].contiguous())\n                        else:\n                            h_new = h_new.contiguous()\n                        new_beams.append((new_seq, new_score, h_new))\n                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n                if beams[0][0][-1].item() == 2:  # Stop if <EOS>\n                    break\n            outputs.append(beams[0][0][1:])  # Exclude <SOS>\n        return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:37:29.153286Z","iopub.execute_input":"2025-05-19T08:37:29.153695Z","iopub.status.idle":"2025-05-19T08:37:29.174719Z","shell.execute_reply.started":"2025-05-19T08:37:29.153675Z","shell.execute_reply":"2025-05-19T08:37:29.174065Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# data prepration class ","metadata":{}},{"cell_type":"code","source":"\n\n\nclass DataPreprocessor:\n    def __init__(self, batch_size=32, device='cpu'):\n        self.batch_size = batch_size\n        self.device = device\n        self.src_vocab = None\n        self.tgt_vocab = None\n        self.PAD_TOKEN = '<PAD>'\n        self.SOS_TOKEN = '<SOS>'\n        self.EOS_TOKEN = '<EOS>'\n        self.UNK_TOKEN = '<UNK>'\n        self.PAD_IDX = 0\n        self.SOS_IDX = 1\n        self.EOS_IDX = 2\n        self.UNK_IDX = 3\n\n    def normalize_string(self, s):\n        s = unicodedata.normalize('NFC', str(s))\n        if all(ord(c) < 128 for c in s):\n            s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n            s = s.lower()\n        return s.strip()\n\n    def load_dataset(self, file_path=None, data_frame=None):\n        if file_path:\n            try:\n                data = pd.read_csv(file_path, sep='\\t', header=None)\n            except:\n                data = pd.read_csv(file_path, header=None)\n        elif data_frame is not None:\n            data = data_frame.copy()\n        else:\n            raise ValueError(\"Either file_path or data_frame must be provided.\")\n        data = data.rename(columns={0: 'tgt', 1: 'src'})\n        data['src'] = data['src'].apply(self.normalize_string)\n        data['tgt'] = data['tgt'].apply(self.normalize_string)\n        return data\n\n    def create_vocab(self, data, column):\n        vocab = {self.PAD_TOKEN: self.PAD_IDX, self.SOS_TOKEN: self.SOS_IDX,\n                 self.EOS_TOKEN: self.EOS_IDX, self.UNK_TOKEN: self.UNK_IDX}\n        for seq in data[column]:\n            if pd.notna(seq):\n                for char in seq:\n                    if char not in vocab:\n                        vocab[char] = len(vocab)\n        return vocab\n\n    def build_vocabularies(self, train_data):\n        self.src_vocab = self.create_vocab(train_data, 'src')\n        self.tgt_vocab = self.create_vocab(train_data, 'tgt')\n        return self.src_vocab, self.tgt_vocab\n\n    class TranslationDataset(Dataset):\n        def __init__(self, data, input_vocab, output_vocab):\n            self.data = data\n            self.input_vocab = input_vocab\n            self.output_vocab = output_vocab\n\n        def __len__(self):\n            return len(self.data)\n\n        def __getitem__(self, idx):\n            src = [self.input_vocab.get(c, self.input_vocab['<UNK>']) for c in self.data.iloc[idx, 1]] + [self.input_vocab['<EOS>']]\n            tgt = [self.output_vocab['<SOS>']] + [self.output_vocab.get(c, self.output_vocab['<UNK>']) for c in self.data.iloc[idx, 0]] + [self.output_vocab['<EOS>']]\n            src_str = self.data.iloc[idx, 1]\n            tgt_str = self.data.iloc[idx, 0]\n            return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long), src_str, tgt_str\n\n    def pad_collate(self, batch):\n        src_batch, tgt_batch, src_strs, tgt_strs = zip(*batch)\n        src_padded = pad_sequence(src_batch, batch_first=True, padding_value=self.PAD_IDX)\n        tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=self.PAD_IDX)\n        return src_padded, tgt_padded, list(src_strs), list(tgt_strs)\n\n    def prepare_data(self, train_data, val_data, test_data):\n        if self.src_vocab is None or self.tgt_vocab is None:\n            self.build_vocabularies(train_data)\n        train_dataset = self.TranslationDataset(train_data, self.src_vocab, self.tgt_vocab)\n        val_dataset = self.TranslationDataset(val_data, self.src_vocab, self.tgt_vocab)\n        test_dataset = self.TranslationDataset(test_data, self.src_vocab, self.tgt_vocab)\n        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True,\n                                 collate_fn=self.pad_collate, pin_memory=True)\n        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False,\n                                collate_fn=self.pad_collate, pin_memory=True)\n        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False,\n                                 collate_fn=self.pad_collate, pin_memory=True)\n        return train_loader, val_loader, test_loader\nimport torch\nimport torch.nn as nn\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:37:35.681917Z","iopub.execute_input":"2025-05-19T08:37:35.682454Z","iopub.status.idle":"2025-05-19T08:37:35.696141Z","shell.execute_reply.started":"2025-05-19T08:37:35.682432Z","shell.execute_reply":"2025-05-19T08:37:35.695525Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# train class ","metadata":{}},{"cell_type":"code","source":"\n# Assuming DataPreprocessor, Encoder, Decoder, Seq2Seq are defined as in your previous code\nclass Trainer:\n    def __init__(self, model, train_loader, val_loader, config, device='cpu', save_path='best_model.pt'):\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.config = config\n        self.teacher_forcing_ratio = config.teacher_forcing\n        self.num_epochs = config.epochs\n        self.save_path = save_path\n        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # Changed to CrossEntropyLoss\n        self.optimizer = optim.Adam(self.model.parameters(), lr=config.learning_rate)\n        self.src_vocab = None  # To store vocab for predictions\n        self.tgt_vocab = None\n\n    def compute_token_accuracy(self, outputs, targets):\n        \"\"\"Compute token-level accuracy.\"\"\"\n        outputs = outputs.argmax(dim=-1)  # [batch_size, seq_len]\n        non_pad_mask = (targets != 0) & (targets != 1) & (targets != 2)  # Exclude <PAD>, <SOS>, <EOS>\n        correct = (outputs == targets) & non_pad_mask\n        total = non_pad_mask.sum().item()\n        correct = correct.sum().item()\n        return correct / total if total > 0 else 0.0\n\n    def compute_sequence_accuracy(self, outputs, targets):\n        \"\"\"Compute sequence-level accuracy.\"\"\"\n        outputs = outputs.argmax(dim=-1)  # [batch_size, seq_len]\n        correct = 0\n        total = outputs.size(0)\n        for pred, tgt in zip(outputs, targets):\n            # Compare sequences, ignoring <PAD>, <SOS>, <EOS>\n            pred = pred[(tgt != 0) & (tgt != 1) & (tgt != 2)]\n            tgt = tgt[(tgt != 0) & (tgt != 1) & (tgt != 2)]\n            if torch.equal(pred, tgt):\n                correct += 1\n        return correct / total if total > 0 else 0.0\n\n    def train_epoch(self):\n        self.model.train()\n        total_loss, total_token_acc, total_seq_acc, total_samples = 0.0, 0.0, 0.0, 0\n\n        pbar = tqdm(self.train_loader, desc=\"Training\", leave=False)\n        for src, tgt, _, _ in pbar:  # Adjusted for src_strs, tgt_strs from DataLoader\n            src, tgt = src.to(self.device), tgt.to(self.device)\n            self.optimizer.zero_grad()\n\n            output = self.model(src, tgt, self.teacher_forcing_ratio)\n            output = output[:, 1:].contiguous().view(-1, output.size(-1))\n            tgt_flat = tgt[:, 1:].contiguous().view(-1)\n\n            loss = self.criterion(output, tgt_flat)\n            loss.backward()\n            self.optimizer.step()\n\n            batch_size = src.size(0)\n            token_acc = self.compute_token_accuracy(\n                output.view(batch_size, -1, output.size(-1)), tgt[:, 1:]\n            )\n            seq_acc = self.compute_sequence_accuracy(\n                output.view(batch_size, -1, output.size(-1)), tgt[:, 1:]\n            )\n\n            total_loss += loss.item() * batch_size\n            total_token_acc += token_acc * batch_size\n            total_seq_acc += seq_acc * batch_size\n            total_samples += batch_size\n\n            pbar.set_postfix(loss=loss.item(), token_acc=token_acc, seq_acc=seq_acc)\n\n        avg_loss = total_loss / total_samples\n        avg_token_acc = total_token_acc / total_samples\n        avg_seq_acc = total_seq_acc / total_samples\n        return avg_loss, avg_token_acc, avg_seq_acc\n\n    def evaluate(self, loader):\n        self.model.eval()\n        total_loss, total_token_acc, total_seq_acc, total_samples = 0.0, 0.0, 0.0, 0\n\n        pbar = tqdm(loader, desc=\"Evaluating\", leave=False)\n        with torch.no_grad():\n            for src, tgt, _, _ in pbar:\n                src, tgt = src.to(self.device), tgt.to(self.device)\n\n                output = self.model(src, tgt, teacher_forcing_ratio=0.0)\n                output = output[:, 1:].contiguous().view(-1, output.size(-1))\n                tgt_flat = tgt[:, 1:].contiguous().view(-1)\n\n                loss = self.criterion(output, tgt_flat)\n\n                batch_size = src.size(0)\n                token_acc = self.compute_token_accuracy(\n                    output.view(batch_size, -1, output.size(-1)), tgt[:, 1:]\n                )\n                seq_acc = self.compute_sequence_accuracy(\n                    output.view(batch_size, -1, output.size(-1)), tgt[:, 1:]\n                )\n\n                total_loss += loss.item() * batch_size\n                total_token_acc += token_acc * batch_size\n                total_seq_acc += seq_acc * batch_size\n                total_samples += batch_size\n\n                pbar.set_postfix(loss=loss.item(), token_acc=token_acc, seq_acc=seq_acc)\n\n        avg_loss = total_loss / total_samples\n        avg_token_acc = total_token_acc / total_samples\n        avg_seq_acc = total_seq_acc / total_samples\n        return avg_loss, avg_token_acc, avg_seq_acc\n\n    def train(self, src_vocab, tgt_vocab):\n        \"\"\"Train the model, logging metrics and predictions to Wandb.\"\"\"\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        best_val_seq_acc = 0.0\n        patience = getattr(self.config, 'patience', 3)\n        patience_counter = 0\n\n        for epoch in range(1, self.num_epochs + 1):\n            # Train\n            train_loss, train_token_acc, train_seq_acc = self.train_epoch()\n            # Evaluate\n            val_loss, val_token_acc, val_seq_acc = self.evaluate(self.val_loader)\n\n            # Print metrics\n            print(f'\\nEpoch {epoch}/{self.num_epochs}')\n            print(f'Train Loss: {train_loss:.4f} | Train Token Acc: {train_token_acc*100:.2f}% | Train Seq Acc: {train_seq_acc*100:.2f}%')\n            print(f'Val Loss:   {val_loss:.4f} | Val Token Acc:   {val_token_acc*100:.2f}% | Val Seq Acc:   {val_seq_acc*100:.2f}%')\n            print('-' * 60)\n\n            # Log metrics to Wandb\n            wandb.log({\n                'epoch': epoch,\n                'train_loss': train_loss,\n                 'val_loss': val_loss,\n                'train_token_accuracy': train_token_acc,\n                'val_token_accuracy': val_token_acc,\n                'train_sequence_accuracy': train_seq_acc,\n                'val_sequence_accuracy': val_seq_acc\n            })\n\n            # Log sample predictions\n            src_sample, tgt_sample, src_strs, tgt_strs = next(iter(self.val_loader))\n            src_sample = src_sample.to(self.device)\n            preds = self.model.predict(src_sample[:5], max_len=30, beam_size=self.config.beam_size)\n\n            inv_src_vocab = {v: k for k, v in src_vocab.items()}\n            inv_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n            table = wandb.Table(columns=[\"Input\", \"Target\", \"Prediction\"])\n            for i in range(len(preds)):\n                input_str = ''.join([inv_src_vocab.get(id.item(), '?') for id in src_sample[i] if id.item() not in [0, src_vocab['<EOS>']]])\n                target_str = ''.join([inv_tgt_vocab.get(id.item(), '?') for id in tgt_sample[i] if id.item() not in [0, tgt_vocab['<EOS>'], tgt_vocab['<SOS>']]])\n                pred_str = ''.join([inv_tgt_vocab.get(id.item(), '?') for id in preds[i] if id.item() not in [0, tgt_vocab['<EOS>']]])\n                table.add_data(input_str, target_str, pred_str)\n            wandb.log({\"predictions\": table})\n\n            # Early stopping and checkpoint\n            if val_seq_acc > best_val_seq_acc:\n                best_val_seq_acc = val_seq_acc\n                patience_counter = 0\n                torch.save(self.model.state_dict(), self.save_path)\n                print(f\"✅ New best model saved with val sequence accuracy: {val_seq_acc*100:.2f}%\")\n            else:\n                patience_counter += 1\n                print(f\"⚠️ No improvement. Patience counter: {patience_counter}/{patience}\")\n                if patience_counter >= patience:\n                    print(\"🛑 Early stopping triggered.\")\n                    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:37:40.202032Z","iopub.execute_input":"2025-05-19T08:37:40.202307Z","iopub.status.idle":"2025-05-19T08:37:40.222444Z","shell.execute_reply.started":"2025-05-19T08:37:40.202286Z","shell.execute_reply":"2025-05-19T08:37:40.221756Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# hyper parameter tuning for searching best hyperparamter","metadata":{}},{"cell_type":"code","source":"import torch\nimport wandb\nimport pandas as pd\nimport os\n\ndef train_loader(\n    train_path='/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv',\n    valid_path='/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv',\n    test_path='/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv',\n    device='cuda' if torch.cuda.is_available() else 'cpu',\n    save_path='/kaggle/working/best_model.pt'\n):\n    \"\"\"\n    Training function for running a WandB sweep on the Bengali Dakshina dataset.\n    \"\"\"\n    # Initialize WandB\n    wandb.init(project=\"assignment_3\")\n    # Shortcut to config\n    config = wandb.config\n\n    # Construct a descriptive run name\n    run_name = (\n        f\"-cell-{config.cell_type}\"\n        f\"embed-{config.emb_dim}\"\n        f\"-enc_layers-{config.enc_layers}\"\n        f\"-dec_layers-{config.dec_layers}\"\n        f\"-hid-{config.hidden_dim}\"\n       \n        f\"-dropout-{config.dropout}\"\n        f\"-bs-{config.batch_size}\"\n        f\"-lr-{config.learning_rate}\"\n        f\"-tf-{config.teacher_forcing}\"\n        f\"-beam-{config.beam_size}\"\n    )\n    wandb.run.name = run_name\n\n    \n    # Initialize DataPreprocessor\n    preprocessor = DataPreprocessor(batch_size=config.batch_size, device=device)\n\n    # Load datasets\n    train_data = preprocessor.load_dataset(train_path)\n    val_data = preprocessor.load_dataset(valid_path)\n    test_data = preprocessor.load_dataset(test_path)\n\n    # Prepare data loaders\n    train_loader, val_loader, test_loader = preprocessor.prepare_data(train_data, val_data, test_data)\n\n    # Initialize model\n    encoder = Encoder(\n        input_size=len(preprocessor.src_vocab),\n        embedding_dim=config.emb_dim,\n        hidden_size=config.hidden_dim,\n        num_layers=config.enc_layers,\n        cell_type=config.cell_type,\n        dropout=config.dropout\n    )\n    decoder = Decoder(\n        output_size=len(preprocessor.tgt_vocab),\n        embedding_dim=config.emb_dim,\n        hidden_size=config.hidden_dim,\n        num_layers=config.dec_layers,\n        cell_type=config.cell_type,\n        dropout=config.dropout\n    )\n    model = Seq2Seq(encoder, decoder).to(device)\n\n    # Initialize Trainer\n    trainer = Trainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        config=config,\n        device=device,\n        save_path=save_path\n    )\n\n    # Train with vocabularies\n    trainer.train(preprocessor.src_vocab, preprocessor.tgt_vocab)\n\n   \n    # Finish Wandb run\n    wandb.finish()\n\n# Wandb sweep config\nsweep_config = {\n    'method': 'bayes',\n    'metric': {'name': 'val_sequence_accuracy', 'goal': 'maximize'},\n    'parameters': {\n        'emb_dim': {'values': [64, 128, 256]},\n        'hidden_dim': {'values': [128, 256]},\n        'enc_layers': {'values': [1, 2, 3]},\n        'dec_layers': {'values': [1, 2, 3]},\n        'cell_type': {'values': ['LSTM', 'GRU','RNN']},\n        'dropout': {'values': [0.2, 0.3, 0.4]},\n        'batch_size': {'values': [32, 64, 128]},\n        'learning_rate': {'values': [0.001, 0.0005, 0.0001]},\n        'teacher_forcing': {'values': [0.5, 0.7, 0.9]},\n        'beam_size': {'values': [1, 3, 5]},\n        'patience': {'value': 3},\n        'epochs': {'values': [10, 15]}\n    }\n}\n\nif __name__ == \"__main__\":\n    sweep_id = wandb.sweep(sweep_config, project=\"assignment_3\")\n    wandb.agent(sweep_id, function=train_loader, count=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:37:45.378922Z","iopub.execute_input":"2025-05-19T08:37:45.379639Z","execution_failed":"2025-05-19T11:48:34.891Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: njnchcyt\nSweep URL: https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fxchzyhd with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'assignment_3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_083753-fxchzyhd</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/fxchzyhd' target=\"_blank\">sparkling-sweep-1</a></strong> to <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/fxchzyhd' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/fxchzyhd</a>"},"metadata":{}},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/15\nTrain Loss: 2.5874 | Train Token Acc: 22.46% | Train Seq Acc: 0.20%\nVal Loss:   2.3123 | Val Token Acc:   27.09% | Val Seq Acc:   1.24%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 1.24%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/15\nTrain Loss: 1.7696 | Train Token Acc: 42.59% | Train Seq Acc: 2.72%\nVal Loss:   1.7507 | Val Token Acc:   43.53% | Val Seq Acc:   6.41%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 6.41%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/15\nTrain Loss: 1.4092 | Train Token Acc: 53.37% | Train Seq Acc: 6.61%\nVal Loss:   1.5791 | Val Token Acc:   49.46% | Val Seq Acc:   9.95%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 9.95%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/15\nTrain Loss: 1.2383 | Train Token Acc: 58.88% | Train Seq Acc: 10.13%\nVal Loss:   1.4740 | Val Token Acc:   52.68% | Val Seq Acc:   13.36%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 13.36%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/15\nTrain Loss: 1.1244 | Train Token Acc: 62.75% | Train Seq Acc: 12.98%\nVal Loss:   1.4283 | Val Token Acc:   55.13% | Val Seq Acc:   15.83%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 15.83%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/15\nTrain Loss: 1.0507 | Train Token Acc: 65.15% | Train Seq Acc: 15.40%\nVal Loss:   1.4045 | Val Token Acc:   55.44% | Val Seq Acc:   16.98%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 16.98%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/15\nTrain Loss: 0.9828 | Train Token Acc: 67.52% | Train Seq Acc: 17.63%\nVal Loss:   1.3845 | Val Token Acc:   57.86% | Val Seq Acc:   19.02%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 19.02%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/15\nTrain Loss: 0.9332 | Train Token Acc: 69.22% | Train Seq Acc: 19.73%\nVal Loss:   1.3425 | Val Token Acc:   58.37% | Val Seq Acc:   20.07%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 20.07%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/15\nTrain Loss: 0.8903 | Train Token Acc: 70.73% | Train Seq Acc: 21.63%\nVal Loss:   1.3455 | Val Token Acc:   59.23% | Val Seq Acc:   21.13%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 21.13%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/15\nTrain Loss: 0.8595 | Train Token Acc: 71.76% | Train Seq Acc: 23.09%\nVal Loss:   1.3046 | Val Token Acc:   60.62% | Val Seq Acc:   22.86%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 22.86%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/15\nTrain Loss: 0.8234 | Train Token Acc: 72.96% | Train Seq Acc: 24.62%\nVal Loss:   1.3099 | Val Token Acc:   60.56% | Val Seq Acc:   22.22%\n------------------------------------------------------------\n⚠️ No improvement. Patience counter: 1/3\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/15\nTrain Loss: 0.7950 | Train Token Acc: 73.91% | Train Seq Acc: 25.97%\nVal Loss:   1.2824 | Val Token Acc:   61.80% | Val Seq Acc:   23.86%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 23.86%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/15\nTrain Loss: 0.7738 | Train Token Acc: 74.72% | Train Seq Acc: 27.39%\nVal Loss:   1.2648 | Val Token Acc:   62.60% | Val Seq Acc:   24.66%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 24.66%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10\nTrain Loss: 1.9526 | Train Token Acc: 39.91% | Train Seq Acc: 1.61%\nVal Loss:   2.3438 | Val Token Acc:   41.19% | Val Seq Acc:   6.41%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 6.41%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10\nTrain Loss: 1.0461 | Train Token Acc: 65.59% | Train Seq Acc: 10.00%\nVal Loss:   2.0366 | Val Token Acc:   50.83% | Val Seq Acc:   13.14%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 13.14%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/10\nTrain Loss: 0.6951 | Train Token Acc: 76.73% | Train Seq Acc: 21.67%\nVal Loss:   1.8632 | Val Token Acc:   58.01% | Val Seq Acc:   20.84%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 20.84%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/10\nTrain Loss: 0.6165 | Train Token Acc: 79.44% | Train Seq Acc: 26.13%\nVal Loss:   1.8727 | Val Token Acc:   58.47% | Val Seq Acc:   21.12%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 21.12%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/10\nTrain Loss: 0.5669 | Train Token Acc: 81.11% | Train Seq Acc: 29.54%\nVal Loss:   1.8266 | Val Token Acc:   60.53% | Val Seq Acc:   24.22%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 24.22%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/10\nTrain Loss: 0.4898 | Train Token Acc: 83.73% | Train Seq Acc: 35.02%\nVal Loss:   1.8048 | Val Token Acc:   61.70% | Val Seq Acc:   25.64%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 25.64%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/10\nTrain Loss: 0.4613 | Train Token Acc: 84.66% | Train Seq Acc: 37.13%\nVal Loss:   1.8065 | Val Token Acc:   62.09% | Val Seq Acc:   26.11%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 26.11%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/10\nTrain Loss: 0.4385 | Train Token Acc: 85.47% | Train Seq Acc: 38.98%\nVal Loss:   1.8159 | Val Token Acc:   63.06% | Val Seq Acc:   27.36%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 27.36%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>train_sequence_accuracy</td><td>▁▃▄▅▆▆▇▇██</td></tr><tr><td>train_token_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>val_sequence_accuracy</td><td>▁▃▅▆▆▇▇▇██</td></tr><tr><td>val_token_accuracy</td><td>▁▄▅▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>0.43852</td></tr><tr><td>train_sequence_accuracy</td><td>0.38979</td></tr><tr><td>train_token_accuracy</td><td>0.85466</td></tr><tr><td>val_loss</td><td>1.81587</td></tr><tr><td>val_sequence_accuracy</td><td>0.27363</td></tr><tr><td>val_token_accuracy</td><td>0.63059</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">-cell-LSTMembed-128-enc_layers-3-dec_layers-1-hid-128-dropout-0.3-bs-32-lr-0.0005-tf-0.9-beam-1</strong> at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/fa3cbt9r' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/fa3cbt9r</a><br> View project at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a><br>Synced 5 W&B file(s), 10 media file(s), 20 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_085749-fa3cbt9r/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qjbguqti with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.9\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'assignment_3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_091644-qjbguqti</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/qjbguqti' target=\"_blank\">faithful-sweep-3</a></strong> to <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/qjbguqti' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/qjbguqti</a>"},"metadata":{}},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/15\nTrain Loss: 2.6691 | Train Token Acc: 19.05% | Train Seq Acc: 0.00%\nVal Loss:   3.7493 | Val Token Acc:   9.25% | Val Seq Acc:   0.00%\n------------------------------------------------------------\n⚠️ No improvement. Patience counter: 1/3\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/15\nTrain Loss: 2.4853 | Train Token Acc: 23.50% | Train Seq Acc: 0.01%\nVal Loss:   3.6128 | Val Token Acc:   10.76% | Val Seq Acc:   0.01%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 0.01%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/15\nTrain Loss: 2.3045 | Train Token Acc: 27.76% | Train Seq Acc: 0.01%\nVal Loss:   3.5799 | Val Token Acc:   10.22% | Val Seq Acc:   0.04%\n------------------------------------------------------------\n⚠️ No improvement. Patience counter: 2/3\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/15\nTrain Loss: 2.2948 | Train Token Acc: 28.06% | Train Seq Acc: 0.02%\nVal Loss:   3.6045 | Val Token Acc:   11.68% | Val Seq Acc:   0.04%\n------------------------------------------------------------\n⚠️ No improvement. Patience counter: 3/3\n🛑 Early stopping triggered.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_loss</td><td>█▅▃▂▂▁▁</td></tr><tr><td>train_sequence_accuracy</td><td>▁▅▄▆▇▄█</td></tr><tr><td>train_token_accuracy</td><td>▁▄▆▇▇██</td></tr><tr><td>val_loss</td><td>█▅▁▃▄▄▅</td></tr><tr><td>val_sequence_accuracy</td><td>▁▃▁█▁██</td></tr><tr><td>val_token_accuracy</td><td>▁▅▇▆▆▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_loss</td><td>2.29481</td></tr><tr><td>train_sequence_accuracy</td><td>0.00019</td></tr><tr><td>train_token_accuracy</td><td>0.28056</td></tr><tr><td>val_loss</td><td>3.60454</td></tr><tr><td>val_sequence_accuracy</td><td>0.00043</td></tr><tr><td>val_token_accuracy</td><td>0.11682</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">-cell-RNNembed-128-enc_layers-2-dec_layers-3-hid-128-dropout-0.4-bs-64-lr-0.001-tf-0.9-beam-1</strong> at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/qjbguqti' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/qjbguqti</a><br> View project at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a><br>Synced 5 W&B file(s), 7 media file(s), 14 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_091644-qjbguqti/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b5rm47n2 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'assignment_3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_092652-b5rm47n2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/b5rm47n2' target=\"_blank\">cosmic-sweep-4</a></strong> to <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/b5rm47n2' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/b5rm47n2</a>"},"metadata":{}},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/15\nTrain Loss: 1.2597 | Train Token Acc: 60.03% | Train Seq Acc: 11.67%\nVal Loss:   1.6352 | Val Token Acc:   54.31% | Val Seq Acc:   15.47%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 15.47%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/15\nTrain Loss: 0.7132 | Train Token Acc: 77.00% | Train Seq Acc: 26.94%\nVal Loss:   1.5811 | Val Token Acc:   59.45% | Val Seq Acc:   21.87%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 21.87%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/15\nTrain Loss: 0.5841 | Train Token Acc: 81.34% | Train Seq Acc: 35.01%\nVal Loss:   1.5891 | Val Token Acc:   60.53% | Val Seq Acc:   23.68%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 23.68%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/15\nTrain Loss: 0.3654 | Train Token Acc: 88.63% | Train Seq Acc: 53.17%\nVal Loss:   1.7184 | Val Token Acc:   62.66% | Val Seq Acc:   24.01%\n------------------------------------------------------------\n⚠️ No improvement. Patience counter: 1/3\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/15\nTrain Loss: 0.3551 | Train Token Acc: 88.95% | Train Seq Acc: 54.32%\nVal Loss:   1.7205 | Val Token Acc:   62.45% | Val Seq Acc:   24.81%\n------------------------------------------------------------\n⚠️ No improvement. Patience counter: 2/3\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/15\nTrain Loss: 0.3390 | Train Token Acc: 89.48% | Train Seq Acc: 55.73%\nVal Loss:   1.7204 | Val Token Acc:   62.88% | Val Seq Acc:   24.78%\n------------------------------------------------------------\n⚠️ No improvement. Patience counter: 3/3\n🛑 Early stopping triggered.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁</td></tr><tr><td>train_sequence_accuracy</td><td>▁▃▅▆▆▇▇▇███</td></tr><tr><td>train_token_accuracy</td><td>▁▅▆▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▄▁▁▃▃▄▆▄███</td></tr><tr><td>val_sequence_accuracy</td><td>▁▅▇▆▇█▇█▇▇▇</td></tr><tr><td>val_token_accuracy</td><td>▁▅▆▆▇▇▇█▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>11</td></tr><tr><td>train_loss</td><td>0.33901</td></tr><tr><td>train_sequence_accuracy</td><td>0.55729</td></tr><tr><td>train_token_accuracy</td><td>0.89482</td></tr><tr><td>val_loss</td><td>1.72045</td></tr><tr><td>val_sequence_accuracy</td><td>0.24776</td></tr><tr><td>val_token_accuracy</td><td>0.62885</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">-cell-GRUembed-256-enc_layers-1-dec_layers-1-hid-256-dropout-0.3-bs-32-lr-0.001-tf-0.7-beam-5</strong> at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/b5rm47n2' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/b5rm47n2</a><br> View project at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a><br>Synced 5 W&B file(s), 11 media file(s), 22 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_092652-b5rm47n2/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mz3ojt1r with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'assignment_3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_094759-mz3ojt1r</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/mz3ojt1r' target=\"_blank\">prime-sweep-5</a></strong> to <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/mz3ojt1r' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/mz3ojt1r</a>"},"metadata":{}},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/15\nTrain Loss: 2.7495 | Train Token Acc: 17.73% | Train Seq Acc: 0.02%\nVal Loss:   2.5031 | Val Token Acc:   24.78% | Val Seq Acc:   0.15%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 0.15%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/15\nTrain Loss: 1.7820 | Train Token Acc: 43.63% | Train Seq Acc: 2.52%\nVal Loss:   1.5914 | Val Token Acc:   48.93% | Val Seq Acc:   8.33%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 8.33%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/15\nTrain Loss: 1.2509 | Train Token Acc: 58.86% | Train Seq Acc: 9.18%\nVal Loss:   1.3802 | Val Token Acc:   56.87% | Val Seq Acc:   16.61%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 16.61%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/15\nTrain Loss: 0.7112 | Train Token Acc: 76.60% | Train Seq Acc: 29.22%\nVal Loss:   1.2144 | Val Token Acc:   65.12% | Val Seq Acc:   26.91%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 26.91%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/15\nTrain Loss: 0.6582 | Train Token Acc: 78.38% | Train Seq Acc: 32.55%\nVal Loss:   1.2132 | Val Token Acc:   65.64% | Val Seq Acc:   28.26%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 28.26%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/15\nTrain Loss: 0.6083 | Train Token Acc: 80.03% | Train Seq Acc: 35.29%\nVal Loss:   1.2240 | Val Token Acc:   66.28% | Val Seq Acc:   29.15%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 29.15%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/15\nTrain Loss: 0.5687 | Train Token Acc: 81.38% | Train Seq Acc: 38.21%\nVal Loss:   1.1666 | Val Token Acc:   67.87% | Val Seq Acc:   31.45%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 31.45%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/15\nTrain Loss: 0.5336 | Train Token Acc: 82.50% | Train Seq Acc: 40.69%\nVal Loss:   1.2108 | Val Token Acc:   67.72% | Val Seq Acc:   30.70%\n------------------------------------------------------------\n⚠️ No improvement. Patience counter: 1/3\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/15\nTrain Loss: 0.5076 | Train Token Acc: 83.33% | Train Seq Acc: 42.28%\nVal Loss:   1.1726 | Val Token Acc:   68.78% | Val Seq Acc:   32.60%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 32.60%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/15\nTrain Loss: 0.4829 | Train Token Acc: 84.18% | Train Seq Acc: 44.39%\nVal Loss:   1.1512 | Val Token Acc:   68.82% | Val Seq Acc:   32.64%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 32.64%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/15\nTrain Loss: 0.4606 | Train Token Acc: 84.92% | Train Seq Acc: 46.12%\nVal Loss:   1.1677 | Val Token Acc:   69.14% | Val Seq Acc:   32.21%\n------------------------------------------------------------\n⚠️ No improvement. Patience counter: 1/3\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/15\nTrain Loss: 0.8689 | Train Token Acc: 70.86% | Train Seq Acc: 14.31%\nVal Loss:   1.7564 | Val Token Acc:   56.73% | Val Seq Acc:   18.22%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 18.22%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/15\nTrain Loss: 0.7378 | Train Token Acc: 75.18% | Train Seq Acc: 19.36%\nVal Loss:   1.6847 | Val Token Acc:   59.75% | Val Seq Acc:   21.50%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 21.50%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/15\nTrain Loss: 0.6467 | Train Token Acc: 78.35% | Train Seq Acc: 23.99%\nVal Loss:   1.6509 | Val Token Acc:   61.68% | Val Seq Acc:   24.57%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 24.57%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/15\nTrain Loss: 0.5752 | Train Token Acc: 80.84% | Train Seq Acc: 28.67%\nVal Loss:   1.6395 | Val Token Acc:   63.15% | Val Seq Acc:   26.60%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 26.60%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/15\nTrain Loss: 0.3703 | Train Token Acc: 87.78% | Train Seq Acc: 45.63%\nVal Loss:   1.6355 | Val Token Acc:   67.01% | Val Seq Acc:   31.58%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 31.58%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/15\nTrain Loss: 0.3467 | Train Token Acc: 88.64% | Train Seq Acc: 48.14%\nVal Loss:   1.6329 | Val Token Acc:   67.39% | Val Seq Acc:   31.84%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 31.84%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/15\nTrain Loss: 0.3223 | Train Token Acc: 89.44% | Train Seq Acc: 50.70%\nVal Loss:   1.6767 | Val Token Acc:   67.69% | Val Seq Acc:   32.58%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 32.58%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15/15\nTrain Loss: 0.3051 | Train Token Acc: 90.00% | Train Seq Acc: 52.69%\nVal Loss:   1.6714 | Val Token Acc:   67.75% | Val Seq Acc:   32.57%\n------------------------------------------------------------\n⚠️ No improvement. Patience counter: 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_loss</td><td>█▅▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_sequence_accuracy</td><td>▁▁▂▃▄▄▅▅▆▆▇▇▇██</td></tr><tr><td>train_token_accuracy</td><td>▁▄▅▆▆▇▇▇▇██████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_sequence_accuracy</td><td>▁▂▄▅▅▆▇▇▇▇█████</td></tr><tr><td>val_token_accuracy</td><td>▁▄▅▆▆▇▇▇███████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_loss</td><td>0.30509</td></tr><tr><td>train_sequence_accuracy</td><td>0.52695</td></tr><tr><td>train_token_accuracy</td><td>0.89997</td></tr><tr><td>val_loss</td><td>1.67143</td></tr><tr><td>val_sequence_accuracy</td><td>0.32568</td></tr><tr><td>val_token_accuracy</td><td>0.67747</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">-cell-LSTMembed-128-enc_layers-3-dec_layers-2-hid-256-dropout-0.4-bs-32-lr-0.0001-tf-0.9-beam-1</strong> at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/w98fsu1b' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/w98fsu1b</a><br> View project at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a><br>Synced 5 W&B file(s), 15 media file(s), 20 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_101245-w98fsu1b/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0d6pqdbq with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.9\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'assignment_3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_104818-0d6pqdbq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/0d6pqdbq' target=\"_blank\">cool-sweep-7</a></strong> to <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/0d6pqdbq' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/0d6pqdbq</a>"},"metadata":{}},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/15\nTrain Loss: 0.6009 | Train Token Acc: 80.01% | Train Seq Acc: 26.74%\nVal Loss:   1.6379 | Val Token Acc:   63.80% | Val Seq Acc:   28.10%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 28.10%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/15\nTrain Loss: 0.5407 | Train Token Acc: 82.05% | Train Seq Acc: 30.94%\nVal Loss:   1.6315 | Val Token Acc:   65.18% | Val Seq Acc:   29.64%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 29.64%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/15\nTrain Loss: 0.4884 | Train Token Acc: 83.87% | Train Seq Acc: 34.92%\nVal Loss:   1.6131 | Val Token Acc:   66.36% | Val Seq Acc:   31.30%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 31.30%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/15\nTrain Loss: 0.4503 | Train Token Acc: 85.15% | Train Seq Acc: 37.97%\nVal Loss:   1.6259 | Val Token Acc:   66.73% | Val Seq Acc:   32.51%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 32.51%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/15\nTrain Loss: 0.4145 | Train Token Acc: 86.31% | Train Seq Acc: 41.03%\nVal Loss:   1.6181 | Val Token Acc:   67.51% | Val Seq Acc:   33.24%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 33.24%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/15\nTrain Loss: 0.3192 | Train Token Acc: 89.51% | Train Seq Acc: 50.65%\nVal Loss:   1.5952 | Val Token Acc:   69.47% | Val Seq Acc:   35.82%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 35.82%\n","output_type":"stream"},{"name":"stderr","text":"Training:  33%|███▎      | 989/2955 [00:46<01:31, 21.56it/s, loss=0.558, seq_acc=0.438, token_acc=0.83] ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\n\nclass TestEvaluator:\n    def __init__(self, model, test_loader, src_vocab, tgt_vocab, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.model = model.to(device)\n        self.test_loader = test_loader\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        self.device = device\n        self.inv_src_vocab = {v: k for k, v in src_vocab.items()}\n        self.inv_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n    def check_model_file(self, model_path):\n        \"\"\"Check if the model file exists and provide guidance if it doesn't.\"\"\"\n        if not os.path.exists(model_path):\n            error_msg = f\"Error: Model file not found at '{model_path}'.\\n\"\n            error_msg += \"Possible solutions:\\n\"\n            error_msg += \"1. Ensure training completed successfully and saved the model to '/kaggle/working/best_model.pt'.\\n\"\n            error_msg += \"2. Check if the model was saved to a different path and update 'model_path'.\\n\"\n            error_msg += \"3. Rerun the training script to generate the model.\\n\"\n            error_msg += \"4. If running in Kaggle, verify that '/kaggle/working/' is accessible and the file was persisted.\\n\"\n            error_msg += \"5. Provide the correct path to an existing model file.\"\n            raise FileNotFoundError(error_msg)\n        print(f\"Model file found at '{model_path}'.\")\n    def compute_sequence_accuracy(self, outputs, targets):\n        \"\"\"Compute sequence-level accuracy (exact match, ignoring special tokens).\"\"\"\n        outputs = outputs.argmax(dim=-1)  # [batch_size, seq_len]\n        correct = 0\n        total = outputs.size(0)\n        for pred, tgt in zip(outputs, targets):\n            pred = pred[(tgt != 0) & (tgt != 1) & (tgt != 2)]  # Exclude <PAD>, <SOS>, <EOS>\n            tgt = tgt[(tgt != 0) & (tgt != 1) & (tgt != 2)]\n            if torch.equal(pred, tgt):\n                correct += 1\n        return correct / total if total > 0 else 0.0\n\n    def evaluate_test_set(self):\n        \"\"\"Evaluate the model on the test set and return sequence accuracy.\"\"\"\n        self.model.eval()\n        total_seq_acc, total_samples = 0.0, 0\n\n        with torch.no_grad():\n            for src, tgt, _, _ in tqdm(self.test_loader, desc=\"Evaluating Test Set\"):\n                src, tgt = src.to(self.device), tgt.to(self.device)\n                output = self.model(src, tgt, teacher_forcing_ratio=0.0)\n                output = output[:, 1:].contiguous()  # Exclude <SOS>\n                seq_acc = self.compute_sequence_accuracy(output, tgt[:, 1:])\n                batch_size = src.size(0)\n                total_seq_acc += seq_acc * batch_size\n                total_samples += batch_size\n\n        avg_seq_acc = total_seq_acc / total_samples\n        return avg_seq_acc\n\n    def generate_predictions(self, output_dir=\"predictions_vanilla\", num_samples=10):\n        \"\"\"Generate predictions for the test set and save them to a file. Return samples for display.\"\"\"\n        self.model.eval()\n        predictions = []\n        sample_data = []\n\n        os.makedirs(output_dir, exist_ok=True)\n        pred_file = os.path.join(output_dir, \"predictions.tsv\")\n\n        with torch.no_grad():\n            for src, tgt, src_strs, tgt_strs in tqdm(self.test_loader, desc=\"Generating Predictions\"):\n                src = src.to(self.device)\n                preds = self.model.predict(src, max_len=30, beam_size=3)  # Use beam_size=3 as default\n                for i in range(len(preds)):\n                    input_str = src_strs[i]\n                    target_str = tgt_strs[i]\n                    pred_ids = preds[i]\n                    pred_str = ''.join([self.inv_tgt_vocab.get(id.item(), '?') for id in pred_ids if id.item() not in [0, self.tgt_vocab['<EOS>']]])\n                    predictions.append((input_str, target_str, pred_str))\n                    if len(sample_data) < num_samples:\n                        sample_data.append((input_str, target_str, pred_str))\n\n        # Save all predictions to a TSV file\n        pred_df = pd.DataFrame(predictions, columns=[\"Input\", \"Target\", \"Prediction\"])\n        pred_df.to_csv(pred_file, sep='\\t', index=False)\n\n        return sample_data, pred_file\n\n    def display_samples(self, sample_data):\n        \"\"\"Format sample predictions as a markdown table.\"\"\"\n        markdown = \"| Input | Target | Prediction | Match |\\n\"\n        markdown += \"|-------|--------|------------|-------|\\n\"\n        for input_str, target_str, pred_str in sample_data:\n            match = \"✅\" if pred_str == target_str else \"❌\"\n            markdown += f\"| {input_str} | {target_str} | {pred_str} | {match} |\\n\"\n        return markdown\n\n    def display_samples_highlight_incorrect(self, sample_data):\n        \"\"\"Format sample predictions as a markdown table, highlighting incorrect predictions.\"\"\"\n        markdown = \"| Input | Target | Prediction | Match |\\n\"\n        markdown += \"|-------|--------|------------|-------|\\n\"\n        for input_str, target_str, pred_str in sample_data:\n            match = \"✅\" if pred_str == target_str else \"❌\"\n            display_pred = f\"**{pred_str}**\" if pred_str != target_str else pred_str\n            markdown += f\"| {input_str} | {target_str} | {display_pred} | {match} |\\n\"\n        return markdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:44:50.324508Z","iopub.execute_input":"2025-05-18T04:44:50.324904Z","iopub.status.idle":"2025-05-18T04:44:50.347652Z","shell.execute_reply.started":"2025-05-18T04:44:50.324873Z","shell.execute_reply":"2025-05-18T04:44:50.347101Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nimport wandb\n\n# Assuming DataPreprocessor, Encoder, Decoder, Seq2Seq, and Trainer are defined as in the original code\n\ndef train_with_best_hyperparams(\n    train_path='/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv',\n    valid_path='/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv',\n    test_path='/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv',\n    device='cuda' if torch.cuda.is_available() else 'cpu',\n    save_path='/kaggle/working/best_model.pt'\n):\n    \"\"\"\n    Train the model with the best hyperparameters and save it to save_path.\n    Returns the trained model and vocabularies.\n    \"\"\"\n    # Best hyperparameters from WandB\n    config = {\n        'batch_size': 64,\n        'beam_size': 1,\n        'cell_type': 'GRU',\n        'dec_layers': 2,\n        'dropout': 0.2,\n        'emb_dim': 256,\n        'enc_layers': 3,\n        'epochs': 1,\n        'hidden_dim': 256,\n        'learning_rate': 0.001,\n        'patience': 3,\n        'teacher_forcing': 0.7\n    }\n\n    # Convert config to an object for compatibility with Trainer\n    class Config:\n        def __init__(self, params):\n            for key, value in params.items():\n                setattr(self, key, value)\n    \n    config_obj = Config(config)\n\n    # Initialize WandB run\n    wandb.init(project=\"transliteration\", config=config, name=\"best_hyperparams_run\")\n    \n    # Initialize DataPreprocessor\n    preprocessor = DataPreprocessor(batch_size=config['batch_size'], device=device)\n    \n    # Load datasets\n    train_data = preprocessor.load_dataset(train_path)\n    val_data = preprocessor.load_dataset(valid_path)\n    test_data = preprocessor.load_dataset(test_path)\n    \n    # Prepare data loaders\n    train_loader, val_loader, test_loader = preprocessor.prepare_data(train_data, val_data, test_data)\n    \n    # Initialize model\n    encoder = Encoder(\n        input_size=len(preprocessor.src_vocab),\n        embedding_dim=config['emb_dim'],\n        hidden_size=config['hidden_dim'],\n        num_layers=config['enc_layers'],\n        cell_type=config['cell_type'],\n        dropout=config['dropout']\n    )\n    decoder = Decoder(\n        output_size=len(preprocessor.tgt_vocab),\n        embedding_dim=config['emb_dim'],\n        hidden_size=config['hidden_dim'],\n        num_layers=config['dec_layers'],\n        cell_type=config['cell_type'],\n        dropout=config['dropout']\n    )\n    model = Seq2Seq(encoder, decoder).to(device)\n    \n    # Initialize Trainer\n    trainer = Trainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        config=config_obj,\n        device=device,\n        save_path=save_path\n    )\n    \n    # Train the model\n    trainer.train(preprocessor.src_vocab, preprocessor.tgt_vocab)\n    \n    # Finish WandB run\n    wandb.finish()\n    \n    print(f\"Training completed. Model saved to {save_path}\")\n    return model, preprocessor.src_vocab, preprocessor.tgt_vocab, test_loader\n\ndef evaluate_with_best_model(\n    model,\n    test_loader,\n    src_vocab,\n    tgt_vocab,\n    model_path='/kaggle/working/best_model.pt',\n    device='cuda' if torch.cuda.is_available() else 'cpu'\n):\n    \"\"\"\n    Evaluate the model on the test set and generate predictions with highlighted incorrect outputs.\n    Returns sequence accuracy, markdown table, and predictions file path.\n    \"\"\"\n    # Initialize TestEvaluator\n    evaluator = TestEvaluator(model, test_loader, src_vocab, tgt_vocab, device)\n    \n    # Check model file\n    evaluator.check_model_file(model_path)\n    \n    # Load the model (already loaded in model, but verify for consistency)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    print(f\"Loaded model from {model_path}\")\n    \n    # Evaluate on test set\n    test_seq_acc = evaluator.evaluate_test_set()\n    print(f\"\\nTest Set Sequence Accuracy: {test_seq_acc*100:.2f}%\")\n    \n    # Generate predictions\n    sample_data, pred_file = evaluator.generate_predictions(num_samples=10)\n    print(f\"\\nPredictions saved to {pred_file}\")\n    print(pred_file.sample(20))\n    # Display sample predictions with incorrect ones highlighted\n    markdown_table = evaluator.display_samples_highlight_incorrect(sample_data)\n    print(\"\\nSample Predictions (Incorrect Predictions Highlighted):\")\n    print(markdown_table)\n    \n    return test_seq_acc, markdown_table, pred_file\n\nif __name__ == \"__main__\":\n    try:\n        # Train with best hyperparameters\n        model, src_vocab, tgt_vocab, test_loader = train_with_best_hyperparams()\n        \n        # Evaluate on test set\n        test_seq_acc, markdown_table, pred_file = evaluate_with_best_model(\n            model, test_loader, src_vocab, tgt_vocab\n        )\n    except FileNotFoundError as e:\n        print(e)\n    except Exception as e:\n        print(f\"Error during training or evaluation: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:51:27.732641Z","iopub.execute_input":"2025-05-18T04:51:27.732920Z","iopub.status.idle":"2025-05-18T04:55:00.129196Z","shell.execute_reply.started":"2025-05-18T04:51:27.732898Z","shell.execute_reply":"2025-05-18T04:55:00.128510Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_045127-ho5az2n0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bgorai005-iit-madras/transliteration/runs/ho5az2n0' target=\"_blank\">best_hyperparams_run</a></strong> to <a href='https://wandb.ai/bgorai005-iit-madras/transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bgorai005-iit-madras/transliteration' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bgorai005-iit-madras/transliteration/runs/ho5az2n0' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/transliteration/runs/ho5az2n0</a>"},"metadata":{}},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/1\nTrain Loss: 1.2208 | Train Token Acc: 61.94% | Train Seq Acc: 14.63%\nVal Loss:   1.5134 | Val Token Acc:   60.00% | Val Seq Acc:   22.58%\n------------------------------------------------------------\n✅ New best model saved with val sequence accuracy: 22.58%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>train_sequence_accuracy</td><td>▁</td></tr><tr><td>train_token_accuracy</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_sequence_accuracy</td><td>▁</td></tr><tr><td>val_token_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_loss</td><td>1.22085</td></tr><tr><td>train_sequence_accuracy</td><td>0.14631</td></tr><tr><td>train_token_accuracy</td><td>0.61935</td></tr><tr><td>val_loss</td><td>1.51341</td></tr><tr><td>val_sequence_accuracy</td><td>0.22578</td></tr><tr><td>val_token_accuracy</td><td>0.60005</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">best_hyperparams_run</strong> at: <a href='https://wandb.ai/bgorai005-iit-madras/transliteration/runs/ho5az2n0' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/transliteration/runs/ho5az2n0</a><br> View project at: <a href='https://wandb.ai/bgorai005-iit-madras/transliteration' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/transliteration</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250518_045127-ho5az2n0/logs</code>"},"metadata":{}},{"name":"stdout","text":"Training completed. Model saved to /kaggle/working/best_model.pt\nModel file found at '/kaggle/working/best_model.pt'.\nLoaded model from /kaggle/working/best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Test Set: 100%|██████████| 145/145 [00:04<00:00, 35.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTest Set Sequence Accuracy: 23.33%\n","output_type":"stream"},{"name":"stderr","text":"Generating Predictions: 100%|██████████| 145/145 [01:52<00:00,  1.28it/s]","output_type":"stream"},{"name":"stdout","text":"\nPredictions saved to predictions_vanilla/predictions.tsv\nError during training or evaluation: 'str' object has no attribute 'sample'\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\n\n# Correctly load the TSV file into a DataFrame\ndf = pd.read_csv('/kaggle/working/predictions_vanilla/predictions.tsv', sep='\\t')\n\n# Display the first few rows\nprint(df.sample(200))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:58:21.711700Z","iopub.execute_input":"2025-05-18T04:58:21.711993Z","iopub.status.idle":"2025-05-18T04:58:21.737042Z","shell.execute_reply.started":"2025-05-18T04:58:21.711961Z","shell.execute_reply":"2025-05-18T04:58:21.736483Z"}},"outputs":[{"name":"stdout","text":"                  Input         Target   Prediction\n6584        mailashtona      মাইলস্টোন    মাইলস্তান\n4524                not            নোট           নট\n7348             rurala          রুরাল         রুলা\n8792              stari         স্টোরি      স্ট্রাই\n7836          sheleshma       শ্লেষ্মা       সেলেসম\n...                 ...            ...          ...\n6686              marbo          মারবো        মার্ব\n8146              sarju           সরযূ       সার্জু\n2267              ginir          গিনির        জিনির\n2441        grambaseeke    গ্রামবাসীকে  গ্রামবিকেষে\n2589  chalachchitrogulo  চলচ্চিত্রগুলো  চলচিত্রগুলো\n\n[200 rows x 3 columns]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}