{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3485474,"sourceType":"datasetVersion","datasetId":2097669}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Necssary libary","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport pandas as pd\nimport unicodedata\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport wandb\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:36:36.184282Z","iopub.execute_input":"2025-05-19T08:36:36.184457Z","iopub.status.idle":"2025-05-19T08:36:36.188899Z","shell.execute_reply.started":"2025-05-19T08:36:36.184441Z","shell.execute_reply":"2025-05-19T08:36:36.188092Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_path = '/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv'\nvalid_path = '/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv'\ntest_path  = '/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:36:36.190195Z","iopub.execute_input":"2025-05-19T08:36:36.190506Z","iopub.status.idle":"2025-05-19T08:36:36.211240Z","shell.execute_reply.started":"2025-05-19T08:36:36.190478Z","shell.execute_reply":"2025-05-19T08:36:36.210365Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"594642013968a68e466138e783dcece6765c43b9\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:37:16.012378Z","iopub.execute_input":"2025-05-19T08:37:16.012911Z","iopub.status.idle":"2025-05-19T08:37:22.426143Z","shell.execute_reply.started":"2025-05-19T08:37:16.012880Z","shell.execute_reply":"2025-05-19T08:37:22.425554Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbgorai005\u001b[0m (\u001b[33mbgorai005-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Encoder, decoder and seq2seq model class","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_size, embedding_dim, hidden_size, num_layers=1, cell_type='LSTM', dropout=0.2):\n        super(Encoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(input_size, embedding_dim)\n        rnn_class = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[cell_type]\n        self.rnn = rnn_class(\n            input_size=embedding_dim,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_seq):\n        embedded = self.dropout(self.embedding(input_seq))\n        batch_size = input_seq.size(0)\n        device = input_seq.device\n        if self.cell_type == 'LSTM':\n            hidden = (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n                     torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n        else:\n            hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        output, hidden = self.rnn(embedded, hidden)\n        return output, hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embedding_dim, hidden_size, num_layers=1, cell_type='LSTM', dropout=0.2):\n        super(Decoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(output_size, embedding_dim)\n        rnn_class = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[cell_type]\n        self.rnn = rnn_class(\n            input_size=embedding_dim,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        self.out = nn.Linear(hidden_size, output_size)\n        self.dropout = nn.Dropout(dropout)\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n    def forward(self, input_char, hidden):\n        embedded = self.dropout(self.embedding(input_char))\n        output, hidden = self.rnn(embedded, hidden)\n        output = self.softmax(self.out(output.squeeze(1)))\n        return output, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_forcing_ratio=0.5):\n        batch_size = source.size(0)\n        target_len = target.size(1)\n        target_vocab_size = self.decoder.embedding.num_embeddings\n        device = source.device\n        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(device)\n        _, hidden = self.encoder(source)\n        if self.encoder.num_layers != self.decoder.num_layers:\n            if self.encoder.cell_type == 'LSTM':\n                h_n, c_n = hidden\n                if self.decoder.num_layers > self.encoder.num_layers:\n                    extra_layers = self.decoder.num_layers - self.encoder.num_layers\n                    extra_h = torch.zeros(extra_layers, batch_size, self.decoder.hidden_size).to(device)\n                    extra_c = torch.zeros(extra_layers, batch_size, self.decoder.hidden_size).to(device)\n                    h_n = torch.cat([h_n, extra_h], dim=0)\n                    c_n = torch.cat([c_n, extra_c], dim=0)\n                else:\n                    h_n = h_n[:self.decoder.num_layers]\n                    c_n = c_n[:self.decoder.num_layers]\n                hidden = (h_n, c_n)\n            else:\n                if self.decoder.num_layers > self.encoder.num_layers:\n                    extra_layers = self.decoder.num_layers - self.encoder.num_layers\n                    extra_h = torch.zeros(extra_layers, batch_size, self.decoder.hidden_size).to(device)\n                    hidden = torch.cat([hidden, extra_h], dim=0)\n                else:\n                    hidden = hidden[:self.decoder.num_layers]\n        decoder_input = target[:, 0].unsqueeze(1)\n        for t in range(1, target_len):\n            output, hidden = self.decoder(decoder_input, hidden)\n            outputs[:, t] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            decoder_input = target[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n        return outputs\n\n    def predict(self, src, max_len=30, beam_size=3):\n        self.eval()\n        batch_size = src.size(0)\n        device = src.device\n        _, hidden = self.encoder(src)\n        outputs = []\n        for i in range(batch_size):\n            if self.encoder.cell_type == 'LSTM':\n                h = hidden[0][:, i:i+1].contiguous()\n                c = hidden[1][:, i:i+1].contiguous()\n                hidden_state = (h, c)\n            else:\n                hidden_state = hidden[:, i:i+1].contiguous()\n            if self.encoder.num_layers != self.decoder.num_layers:\n                if self.encoder.cell_type == 'LSTM':\n                    h_n, c_n = hidden_state\n                    if self.decoder.num_layers > self.encoder.num_layers:\n                        extra_layers = self.decoder.num_layers - self.encoder.num_layers\n                        extra_h = torch.zeros(extra_layers, 1, self.decoder.hidden_size).to(device)\n                        extra_c = torch.zeros(extra_layers, 1, self.decoder.hidden_size).to(device)\n                        h_n = torch.cat([h_n, extra_h], dim=0)\n                        c_n = torch.cat([c_n, extra_c], dim=0)\n                    else:\n                        h_n = h_n[:self.decoder.num_layers]\n                        c_n = c_n[:self.decoder.num_layers]\n                    hidden_state = (h_n, c_n)\n                else:\n                    if self.decoder.num_layers > self.encoder.num_layers:\n                        extra_layers = self.decoder.num_layers - self.encoder.num_layers\n                        extra_h = torch.zeros(extra_layers, 1, self.decoder.hidden_size).to(device)\n                        hidden_state = torch.cat([hidden_state, extra_h], dim=0)\n                    else:\n                        hidden_state = hidden_state[:self.decoder.num_layers]\n            beams = [(torch.tensor([1], device=device), 0.0, hidden_state)]  # [sequence, score, hidden]\n            for _ in range(max_len):\n                new_beams = []\n                for seq, score, h in beams:\n                    input_char = seq[-1].unsqueeze(0).unsqueeze(0)\n                    output, h_new = self.decoder(input_char, h)\n                    probs = torch.log_softmax(output, dim=-1).squeeze(0)\n                    topk = torch.topk(probs, beam_size)\n                    for idx, prob in zip(topk.indices, topk.values):\n                        new_seq = torch.cat([seq, idx.unsqueeze(0)])\n                        new_score = score + prob.item()\n                        if self.decoder.cell_type == 'LSTM':\n                            h_new = (h_new[0].contiguous(), h_new[1].contiguous())\n                        else:\n                            h_new = h_new.contiguous()\n                        new_beams.append((new_seq, new_score, h_new))\n                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n                if beams[0][0][-1].item() == 2:  # Stop if <EOS>\n                    break\n            outputs.append(beams[0][0][1:])  # Exclude <SOS>\n        return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:37:29.153286Z","iopub.execute_input":"2025-05-19T08:37:29.153695Z","iopub.status.idle":"2025-05-19T08:37:29.174719Z","shell.execute_reply.started":"2025-05-19T08:37:29.153675Z","shell.execute_reply":"2025-05-19T08:37:29.174065Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# data prepration class ","metadata":{}},{"cell_type":"code","source":"\n\n\nclass DataPreprocessor:\n    def __init__(self, batch_size=32, device='cpu'):\n        self.batch_size = batch_size\n        self.device = device\n        self.src_vocab = None\n        self.tgt_vocab = None\n        self.PAD_TOKEN = '<PAD>'\n        self.SOS_TOKEN = '<SOS>'\n        self.EOS_TOKEN = '<EOS>'\n        self.UNK_TOKEN = '<UNK>'\n        self.PAD_IDX = 0\n        self.SOS_IDX = 1\n        self.EOS_IDX = 2\n        self.UNK_IDX = 3\n\n    def normalize_string(self, s):\n        s = unicodedata.normalize('NFC', str(s))\n        if all(ord(c) < 128 for c in s):\n            s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n            s = s.lower()\n        return s.strip()\n\n    def load_dataset(self, file_path=None, data_frame=None):\n        if file_path:\n            try:\n                data = pd.read_csv(file_path, sep='\\t', header=None)\n            except:\n                data = pd.read_csv(file_path, header=None)\n        elif data_frame is not None:\n            data = data_frame.copy()\n        else:\n            raise ValueError(\"Either file_path or data_frame must be provided.\")\n        data = data.rename(columns={0: 'tgt', 1: 'src'})\n        data['src'] = data['src'].apply(self.normalize_string)\n        data['tgt'] = data['tgt'].apply(self.normalize_string)\n        return data\n\n    def create_vocab(self, data, column):\n        vocab = {self.PAD_TOKEN: self.PAD_IDX, self.SOS_TOKEN: self.SOS_IDX,\n                 self.EOS_TOKEN: self.EOS_IDX, self.UNK_TOKEN: self.UNK_IDX}\n        for seq in data[column]:\n            if pd.notna(seq):\n                for char in seq:\n                    if char not in vocab:\n                        vocab[char] = len(vocab)\n        return vocab\n\n    def build_vocabularies(self, train_data):\n        self.src_vocab = self.create_vocab(train_data, 'src')\n        self.tgt_vocab = self.create_vocab(train_data, 'tgt')\n        return self.src_vocab, self.tgt_vocab\n\n    class TranslationDataset(Dataset):\n        def __init__(self, data, input_vocab, output_vocab):\n            self.data = data\n            self.input_vocab = input_vocab\n            self.output_vocab = output_vocab\n\n        def __len__(self):\n            return len(self.data)\n\n        def __getitem__(self, idx):\n            src = [self.input_vocab.get(c, self.input_vocab['<UNK>']) for c in self.data.iloc[idx, 1]] + [self.input_vocab['<EOS>']]\n            tgt = [self.output_vocab['<SOS>']] + [self.output_vocab.get(c, self.output_vocab['<UNK>']) for c in self.data.iloc[idx, 0]] + [self.output_vocab['<EOS>']]\n            src_str = self.data.iloc[idx, 1]\n            tgt_str = self.data.iloc[idx, 0]\n            return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long), src_str, tgt_str\n\n    def pad_collate(self, batch):\n        src_batch, tgt_batch, src_strs, tgt_strs = zip(*batch)\n        src_padded = pad_sequence(src_batch, batch_first=True, padding_value=self.PAD_IDX)\n        tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=self.PAD_IDX)\n        return src_padded, tgt_padded, list(src_strs), list(tgt_strs)\n\n    def prepare_data(self, train_data, val_data, test_data):\n        if self.src_vocab is None or self.tgt_vocab is None:\n            self.build_vocabularies(train_data)\n        train_dataset = self.TranslationDataset(train_data, self.src_vocab, self.tgt_vocab)\n        val_dataset = self.TranslationDataset(val_data, self.src_vocab, self.tgt_vocab)\n        test_dataset = self.TranslationDataset(test_data, self.src_vocab, self.tgt_vocab)\n        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True,\n                                 collate_fn=self.pad_collate, pin_memory=True)\n        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False,\n                                collate_fn=self.pad_collate, pin_memory=True)\n        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False,\n                                 collate_fn=self.pad_collate, pin_memory=True)\n        return train_loader, val_loader, test_loader\nimport torch\nimport torch.nn as nn\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:37:35.681917Z","iopub.execute_input":"2025-05-19T08:37:35.682454Z","iopub.status.idle":"2025-05-19T08:37:35.696141Z","shell.execute_reply.started":"2025-05-19T08:37:35.682432Z","shell.execute_reply":"2025-05-19T08:37:35.695525Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# train class ","metadata":{}},{"cell_type":"code","source":"\n# Assuming DataPreprocessor, Encoder, Decoder, Seq2Seq are defined as in your previous code\nclass Trainer:\n    def __init__(self, model, train_loader, val_loader, config, device='cpu', save_path='best_model.pt'):\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.config = config\n        self.teacher_forcing_ratio = config.teacher_forcing\n        self.num_epochs = config.epochs\n        self.save_path = save_path\n        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # Changed to CrossEntropyLoss\n        self.optimizer = optim.Adam(self.model.parameters(), lr=config.learning_rate)\n        self.src_vocab = None  # To store vocab for predictions\n        self.tgt_vocab = None\n\n    def compute_token_accuracy(self, outputs, targets):\n        \"\"\"Compute token-level accuracy.\"\"\"\n        outputs = outputs.argmax(dim=-1)  # [batch_size, seq_len]\n        non_pad_mask = (targets != 0) & (targets != 1) & (targets != 2)  # Exclude <PAD>, <SOS>, <EOS>\n        correct = (outputs == targets) & non_pad_mask\n        total = non_pad_mask.sum().item()\n        correct = correct.sum().item()\n        return correct / total if total > 0 else 0.0\n\n    def compute_sequence_accuracy(self, outputs, targets):\n        \"\"\"Compute sequence-level accuracy.\"\"\"\n        outputs = outputs.argmax(dim=-1)  # [batch_size, seq_len]\n        correct = 0\n        total = outputs.size(0)\n        for pred, tgt in zip(outputs, targets):\n            # Compare sequences, ignoring <PAD>, <SOS>, <EOS>\n            pred = pred[(tgt != 0) & (tgt != 1) & (tgt != 2)]\n            tgt = tgt[(tgt != 0) & (tgt != 1) & (tgt != 2)]\n            if torch.equal(pred, tgt):\n                correct += 1\n        return correct / total if total > 0 else 0.0\n\n    def train_epoch(self):\n        self.model.train()\n        total_loss, total_token_acc, total_seq_acc, total_samples = 0.0, 0.0, 0.0, 0\n\n        pbar = tqdm(self.train_loader, desc=\"Training\", leave=False)\n        for src, tgt, _, _ in pbar:  # Adjusted for src_strs, tgt_strs from DataLoader\n            src, tgt = src.to(self.device), tgt.to(self.device)\n            self.optimizer.zero_grad()\n\n            output = self.model(src, tgt, self.teacher_forcing_ratio)\n            output = output[:, 1:].contiguous().view(-1, output.size(-1))\n            tgt_flat = tgt[:, 1:].contiguous().view(-1)\n\n            loss = self.criterion(output, tgt_flat)\n            loss.backward()\n            self.optimizer.step()\n\n            batch_size = src.size(0)\n            token_acc = self.compute_token_accuracy(\n                output.view(batch_size, -1, output.size(-1)), tgt[:, 1:]\n            )\n            seq_acc = self.compute_sequence_accuracy(\n                output.view(batch_size, -1, output.size(-1)), tgt[:, 1:]\n            )\n\n            total_loss += loss.item() * batch_size\n            total_token_acc += token_acc * batch_size\n            total_seq_acc += seq_acc * batch_size\n            total_samples += batch_size\n\n            pbar.set_postfix(loss=loss.item(), token_acc=token_acc, seq_acc=seq_acc)\n\n        avg_loss = total_loss / total_samples\n        avg_token_acc = total_token_acc / total_samples\n        avg_seq_acc = total_seq_acc / total_samples\n        return avg_loss, avg_token_acc, avg_seq_acc\n\n    def evaluate(self, loader):\n        self.model.eval()\n        total_loss, total_token_acc, total_seq_acc, total_samples = 0.0, 0.0, 0.0, 0\n\n        pbar = tqdm(loader, desc=\"Evaluating\", leave=False)\n        with torch.no_grad():\n            for src, tgt, _, _ in pbar:\n                src, tgt = src.to(self.device), tgt.to(self.device)\n\n                output = self.model(src, tgt, teacher_forcing_ratio=0.0)\n                output = output[:, 1:].contiguous().view(-1, output.size(-1))\n                tgt_flat = tgt[:, 1:].contiguous().view(-1)\n\n                loss = self.criterion(output, tgt_flat)\n\n                batch_size = src.size(0)\n                token_acc = self.compute_token_accuracy(\n                    output.view(batch_size, -1, output.size(-1)), tgt[:, 1:]\n                )\n                seq_acc = self.compute_sequence_accuracy(\n                    output.view(batch_size, -1, output.size(-1)), tgt[:, 1:]\n                )\n\n                total_loss += loss.item() * batch_size\n                total_token_acc += token_acc * batch_size\n                total_seq_acc += seq_acc * batch_size\n                total_samples += batch_size\n\n                pbar.set_postfix(loss=loss.item(), token_acc=token_acc, seq_acc=seq_acc)\n\n        avg_loss = total_loss / total_samples\n        avg_token_acc = total_token_acc / total_samples\n        avg_seq_acc = total_seq_acc / total_samples\n        return avg_loss, avg_token_acc, avg_seq_acc\n\n    def train(self, src_vocab, tgt_vocab):\n        \"\"\"Train the model, logging metrics and predictions to Wandb.\"\"\"\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        best_val_seq_acc = 0.0\n        patience = getattr(self.config, 'patience', 3)\n        patience_counter = 0\n\n        for epoch in range(1, self.num_epochs + 1):\n            # Train\n            train_loss, train_token_acc, train_seq_acc = self.train_epoch()\n            # Evaluate\n            val_loss, val_token_acc, val_seq_acc = self.evaluate(self.val_loader)\n\n            # Print metrics\n            print(f'\\nEpoch {epoch}/{self.num_epochs}')\n            print(f'Train Loss: {train_loss:.4f} | Train Token Acc: {train_token_acc*100:.2f}% | Train Seq Acc: {train_seq_acc*100:.2f}%')\n            print(f'Val Loss:   {val_loss:.4f} | Val Token Acc:   {val_token_acc*100:.2f}% | Val Seq Acc:   {val_seq_acc*100:.2f}%')\n            print('-' * 60)\n\n            # Log metrics to Wandb\n            wandb.log({\n                'epoch': epoch,\n                'train_loss': train_loss,\n                 'val_loss': val_loss,\n                'train_token_accuracy': train_token_acc,\n                'val_token_accuracy': val_token_acc,\n                'train_sequence_accuracy': train_seq_acc,\n                'val_sequence_accuracy': val_seq_acc\n            })\n\n            # Log sample predictions\n            src_sample, tgt_sample, src_strs, tgt_strs = next(iter(self.val_loader))\n            src_sample = src_sample.to(self.device)\n            preds = self.model.predict(src_sample[:5], max_len=30, beam_size=self.config.beam_size)\n\n            inv_src_vocab = {v: k for k, v in src_vocab.items()}\n            inv_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n            table = wandb.Table(columns=[\"Input\", \"Target\", \"Prediction\"])\n            for i in range(len(preds)):\n                input_str = ''.join([inv_src_vocab.get(id.item(), '?') for id in src_sample[i] if id.item() not in [0, src_vocab['<EOS>']]])\n                target_str = ''.join([inv_tgt_vocab.get(id.item(), '?') for id in tgt_sample[i] if id.item() not in [0, tgt_vocab['<EOS>'], tgt_vocab['<SOS>']]])\n                pred_str = ''.join([inv_tgt_vocab.get(id.item(), '?') for id in preds[i] if id.item() not in [0, tgt_vocab['<EOS>']]])\n                table.add_data(input_str, target_str, pred_str)\n            wandb.log({\"predictions\": table})\n\n            # Early stopping and checkpoint\n            if val_seq_acc > best_val_seq_acc:\n                best_val_seq_acc = val_seq_acc\n                patience_counter = 0\n                torch.save(self.model.state_dict(), self.save_path)\n                print(f\"‚úÖ New best model saved with val sequence accuracy: {val_seq_acc*100:.2f}%\")\n            else:\n                patience_counter += 1\n                print(f\"‚ö†Ô∏è No improvement. Patience counter: {patience_counter}/{patience}\")\n                if patience_counter >= patience:\n                    print(\"üõë Early stopping triggered.\")\n                    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:37:40.202032Z","iopub.execute_input":"2025-05-19T08:37:40.202307Z","iopub.status.idle":"2025-05-19T08:37:40.222444Z","shell.execute_reply.started":"2025-05-19T08:37:40.202286Z","shell.execute_reply":"2025-05-19T08:37:40.221756Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# hyper parameter tuning for searching best hyperparamter","metadata":{}},{"cell_type":"code","source":"import torch\nimport wandb\nimport pandas as pd\nimport os\n\ndef train_loader(\n    train_path='/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv',\n    valid_path='/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv',\n    test_path='/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv',\n    device='cuda' if torch.cuda.is_available() else 'cpu',\n    save_path='/kaggle/working/best_model.pt'\n):\n    \"\"\"\n    Training function for running a WandB sweep on the Bengali Dakshina dataset.\n    \"\"\"\n    # Initialize WandB\n    wandb.init(project=\"assignment_3\")\n    # Shortcut to config\n    config = wandb.config\n\n    # Construct a descriptive run name\n    run_name = (\n        f\"-cell-{config.cell_type}\"\n        f\"embed-{config.emb_dim}\"\n        f\"-enc_layers-{config.enc_layers}\"\n        f\"-dec_layers-{config.dec_layers}\"\n        f\"-hid-{config.hidden_dim}\"\n       \n        f\"-dropout-{config.dropout}\"\n        f\"-bs-{config.batch_size}\"\n        f\"-lr-{config.learning_rate}\"\n        f\"-tf-{config.teacher_forcing}\"\n        f\"-beam-{config.beam_size}\"\n    )\n    wandb.run.name = run_name\n\n    \n    # Initialize DataPreprocessor\n    preprocessor = DataPreprocessor(batch_size=config.batch_size, device=device)\n\n    # Load datasets\n    train_data = preprocessor.load_dataset(train_path)\n    val_data = preprocessor.load_dataset(valid_path)\n    test_data = preprocessor.load_dataset(test_path)\n\n    # Prepare data loaders\n    train_loader, val_loader, test_loader = preprocessor.prepare_data(train_data, val_data, test_data)\n\n    # Initialize model\n    encoder = Encoder(\n        input_size=len(preprocessor.src_vocab),\n        embedding_dim=config.emb_dim,\n        hidden_size=config.hidden_dim,\n        num_layers=config.enc_layers,\n        cell_type=config.cell_type,\n        dropout=config.dropout\n    )\n    decoder = Decoder(\n        output_size=len(preprocessor.tgt_vocab),\n        embedding_dim=config.emb_dim,\n        hidden_size=config.hidden_dim,\n        num_layers=config.dec_layers,\n        cell_type=config.cell_type,\n        dropout=config.dropout\n    )\n    model = Seq2Seq(encoder, decoder).to(device)\n\n    # Initialize Trainer\n    trainer = Trainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        config=config,\n        device=device,\n        save_path=save_path\n    )\n\n    # Train with vocabularies\n    trainer.train(preprocessor.src_vocab, preprocessor.tgt_vocab)\n\n   \n    # Finish Wandb run\n    wandb.finish()\n\n# Wandb sweep config\nsweep_config = {\n    'method': 'bayes',\n    'metric': {'name': 'val_sequence_accuracy', 'goal': 'maximize'},\n    'parameters': {\n        'emb_dim': {'values': [64, 128, 256]},\n        'hidden_dim': {'values': [128, 256]},\n        'enc_layers': {'values': [1, 2, 3]},\n        'dec_layers': {'values': [1, 2, 3]},\n        'cell_type': {'values': ['LSTM', 'GRU','RNN']},\n        'dropout': {'values': [0.2, 0.3, 0.4]},\n        'batch_size': {'values': [32, 64, 128]},\n        'learning_rate': {'values': [0.001, 0.0005, 0.0001]},\n        'teacher_forcing': {'values': [0.5, 0.7, 0.9]},\n        'beam_size': {'values': [1, 3, 5]},\n        'patience': {'value': 3},\n        'epochs': {'values': [10, 15]}\n    }\n}\n\nif __name__ == \"__main__\":\n    sweep_id = wandb.sweep(sweep_config, project=\"assignment_3\")\n    wandb.agent(sweep_id, function=train_loader, count=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:37:45.378922Z","iopub.execute_input":"2025-05-19T08:37:45.379639Z","execution_failed":"2025-05-19T11:48:34.891Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: njnchcyt\nSweep URL: https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fxchzyhd with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'assignment_3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_083753-fxchzyhd</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/fxchzyhd' target=\"_blank\">sparkling-sweep-1</a></strong> to <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/fxchzyhd' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/fxchzyhd</a>"},"metadata":{}},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/15\nTrain Loss: 2.5874 | Train Token Acc: 22.46% | Train Seq Acc: 0.20%\nVal Loss:   2.3123 | Val Token Acc:   27.09% | Val Seq Acc:   1.24%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 1.24%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/15\nTrain Loss: 1.7696 | Train Token Acc: 42.59% | Train Seq Acc: 2.72%\nVal Loss:   1.7507 | Val Token Acc:   43.53% | Val Seq Acc:   6.41%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 6.41%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/15\nTrain Loss: 1.4092 | Train Token Acc: 53.37% | Train Seq Acc: 6.61%\nVal Loss:   1.5791 | Val Token Acc:   49.46% | Val Seq Acc:   9.95%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 9.95%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/15\nTrain Loss: 1.2383 | Train Token Acc: 58.88% | Train Seq Acc: 10.13%\nVal Loss:   1.4740 | Val Token Acc:   52.68% | Val Seq Acc:   13.36%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 13.36%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/15\nTrain Loss: 1.1244 | Train Token Acc: 62.75% | Train Seq Acc: 12.98%\nVal Loss:   1.4283 | Val Token Acc:   55.13% | Val Seq Acc:   15.83%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 15.83%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/15\nTrain Loss: 1.0507 | Train Token Acc: 65.15% | Train Seq Acc: 15.40%\nVal Loss:   1.4045 | Val Token Acc:   55.44% | Val Seq Acc:   16.98%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 16.98%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/15\nTrain Loss: 0.9828 | Train Token Acc: 67.52% | Train Seq Acc: 17.63%\nVal Loss:   1.3845 | Val Token Acc:   57.86% | Val Seq Acc:   19.02%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 19.02%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/15\nTrain Loss: 0.9332 | Train Token Acc: 69.22% | Train Seq Acc: 19.73%\nVal Loss:   1.3425 | Val Token Acc:   58.37% | Val Seq Acc:   20.07%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 20.07%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/15\nTrain Loss: 0.8903 | Train Token Acc: 70.73% | Train Seq Acc: 21.63%\nVal Loss:   1.3455 | Val Token Acc:   59.23% | Val Seq Acc:   21.13%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 21.13%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/15\nTrain Loss: 0.8595 | Train Token Acc: 71.76% | Train Seq Acc: 23.09%\nVal Loss:   1.3046 | Val Token Acc:   60.62% | Val Seq Acc:   22.86%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 22.86%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/15\nTrain Loss: 0.8234 | Train Token Acc: 72.96% | Train Seq Acc: 24.62%\nVal Loss:   1.3099 | Val Token Acc:   60.56% | Val Seq Acc:   22.22%\n------------------------------------------------------------\n‚ö†Ô∏è No improvement. Patience counter: 1/3\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/15\nTrain Loss: 0.7950 | Train Token Acc: 73.91% | Train Seq Acc: 25.97%\nVal Loss:   1.2824 | Val Token Acc:   61.80% | Val Seq Acc:   23.86%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 23.86%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/15\nTrain Loss: 0.7738 | Train Token Acc: 74.72% | Train Seq Acc: 27.39%\nVal Loss:   1.2648 | Val Token Acc:   62.60% | Val Seq Acc:   24.66%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 24.66%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10\nTrain Loss: 1.9526 | Train Token Acc: 39.91% | Train Seq Acc: 1.61%\nVal Loss:   2.3438 | Val Token Acc:   41.19% | Val Seq Acc:   6.41%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 6.41%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10\nTrain Loss: 1.0461 | Train Token Acc: 65.59% | Train Seq Acc: 10.00%\nVal Loss:   2.0366 | Val Token Acc:   50.83% | Val Seq Acc:   13.14%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 13.14%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/10\nTrain Loss: 0.6951 | Train Token Acc: 76.73% | Train Seq Acc: 21.67%\nVal Loss:   1.8632 | Val Token Acc:   58.01% | Val Seq Acc:   20.84%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 20.84%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/10\nTrain Loss: 0.6165 | Train Token Acc: 79.44% | Train Seq Acc: 26.13%\nVal Loss:   1.8727 | Val Token Acc:   58.47% | Val Seq Acc:   21.12%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 21.12%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/10\nTrain Loss: 0.5669 | Train Token Acc: 81.11% | Train Seq Acc: 29.54%\nVal Loss:   1.8266 | Val Token Acc:   60.53% | Val Seq Acc:   24.22%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 24.22%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/10\nTrain Loss: 0.4898 | Train Token Acc: 83.73% | Train Seq Acc: 35.02%\nVal Loss:   1.8048 | Val Token Acc:   61.70% | Val Seq Acc:   25.64%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 25.64%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/10\nTrain Loss: 0.4613 | Train Token Acc: 84.66% | Train Seq Acc: 37.13%\nVal Loss:   1.8065 | Val Token Acc:   62.09% | Val Seq Acc:   26.11%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 26.11%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/10\nTrain Loss: 0.4385 | Train Token Acc: 85.47% | Train Seq Acc: 38.98%\nVal Loss:   1.8159 | Val Token Acc:   63.06% | Val Seq Acc:   27.36%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 27.36%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà</td></tr><tr><td>train_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train_sequence_accuracy</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train_token_accuracy</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_sequence_accuracy</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val_token_accuracy</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>0.43852</td></tr><tr><td>train_sequence_accuracy</td><td>0.38979</td></tr><tr><td>train_token_accuracy</td><td>0.85466</td></tr><tr><td>val_loss</td><td>1.81587</td></tr><tr><td>val_sequence_accuracy</td><td>0.27363</td></tr><tr><td>val_token_accuracy</td><td>0.63059</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">-cell-LSTMembed-128-enc_layers-3-dec_layers-1-hid-128-dropout-0.3-bs-32-lr-0.0005-tf-0.9-beam-1</strong> at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/fa3cbt9r' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/fa3cbt9r</a><br> View project at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a><br>Synced 5 W&B file(s), 10 media file(s), 20 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_085749-fa3cbt9r/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qjbguqti with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.9\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'assignment_3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_091644-qjbguqti</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/qjbguqti' target=\"_blank\">faithful-sweep-3</a></strong> to <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/qjbguqti' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/qjbguqti</a>"},"metadata":{}},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/15\nTrain Loss: 2.6691 | Train Token Acc: 19.05% | Train Seq Acc: 0.00%\nVal Loss:   3.7493 | Val Token Acc:   9.25% | Val Seq Acc:   0.00%\n------------------------------------------------------------\n‚ö†Ô∏è No improvement. Patience counter: 1/3\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/15\nTrain Loss: 2.4853 | Train Token Acc: 23.50% | Train Seq Acc: 0.01%\nVal Loss:   3.6128 | Val Token Acc:   10.76% | Val Seq Acc:   0.01%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 0.01%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/15\nTrain Loss: 2.3045 | Train Token Acc: 27.76% | Train Seq Acc: 0.01%\nVal Loss:   3.5799 | Val Token Acc:   10.22% | Val Seq Acc:   0.04%\n------------------------------------------------------------\n‚ö†Ô∏è No improvement. Patience counter: 2/3\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/15\nTrain Loss: 2.2948 | Train Token Acc: 28.06% | Train Seq Acc: 0.02%\nVal Loss:   3.6045 | Val Token Acc:   11.68% | Val Seq Acc:   0.04%\n------------------------------------------------------------\n‚ö†Ô∏è No improvement. Patience counter: 3/3\nüõë Early stopping triggered.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà</td></tr><tr><td>train_loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train_sequence_accuracy</td><td>‚ñÅ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñà</td></tr><tr><td>train_token_accuracy</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ</td></tr><tr><td>val_sequence_accuracy</td><td>‚ñÅ‚ñÉ‚ñÅ‚ñà‚ñÅ‚ñà‚ñà</td></tr><tr><td>val_token_accuracy</td><td>‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_loss</td><td>2.29481</td></tr><tr><td>train_sequence_accuracy</td><td>0.00019</td></tr><tr><td>train_token_accuracy</td><td>0.28056</td></tr><tr><td>val_loss</td><td>3.60454</td></tr><tr><td>val_sequence_accuracy</td><td>0.00043</td></tr><tr><td>val_token_accuracy</td><td>0.11682</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">-cell-RNNembed-128-enc_layers-2-dec_layers-3-hid-128-dropout-0.4-bs-64-lr-0.001-tf-0.9-beam-1</strong> at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/qjbguqti' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/qjbguqti</a><br> View project at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a><br>Synced 5 W&B file(s), 7 media file(s), 14 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_091644-qjbguqti/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b5rm47n2 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'assignment_3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_092652-b5rm47n2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/b5rm47n2' target=\"_blank\">cosmic-sweep-4</a></strong> to <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/b5rm47n2' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/b5rm47n2</a>"},"metadata":{}},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/15\nTrain Loss: 1.2597 | Train Token Acc: 60.03% | Train Seq Acc: 11.67%\nVal Loss:   1.6352 | Val Token Acc:   54.31% | Val Seq Acc:   15.47%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 15.47%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/15\nTrain Loss: 0.7132 | Train Token Acc: 77.00% | Train Seq Acc: 26.94%\nVal Loss:   1.5811 | Val Token Acc:   59.45% | Val Seq Acc:   21.87%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 21.87%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/15\nTrain Loss: 0.5841 | Train Token Acc: 81.34% | Train Seq Acc: 35.01%\nVal Loss:   1.5891 | Val Token Acc:   60.53% | Val Seq Acc:   23.68%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 23.68%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/15\nTrain Loss: 0.3654 | Train Token Acc: 88.63% | Train Seq Acc: 53.17%\nVal Loss:   1.7184 | Val Token Acc:   62.66% | Val Seq Acc:   24.01%\n------------------------------------------------------------\n‚ö†Ô∏è No improvement. Patience counter: 1/3\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/15\nTrain Loss: 0.3551 | Train Token Acc: 88.95% | Train Seq Acc: 54.32%\nVal Loss:   1.7205 | Val Token Acc:   62.45% | Val Seq Acc:   24.81%\n------------------------------------------------------------\n‚ö†Ô∏è No improvement. Patience counter: 2/3\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/15\nTrain Loss: 0.3390 | Train Token Acc: 89.48% | Train Seq Acc: 55.73%\nVal Loss:   1.7204 | Val Token Acc:   62.88% | Val Seq Acc:   24.78%\n------------------------------------------------------------\n‚ö†Ô∏è No improvement. Patience counter: 3/3\nüõë Early stopping triggered.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>train_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train_sequence_accuracy</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train_token_accuracy</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñà‚ñà‚ñà</td></tr><tr><td>val_sequence_accuracy</td><td>‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>val_token_accuracy</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>11</td></tr><tr><td>train_loss</td><td>0.33901</td></tr><tr><td>train_sequence_accuracy</td><td>0.55729</td></tr><tr><td>train_token_accuracy</td><td>0.89482</td></tr><tr><td>val_loss</td><td>1.72045</td></tr><tr><td>val_sequence_accuracy</td><td>0.24776</td></tr><tr><td>val_token_accuracy</td><td>0.62885</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">-cell-GRUembed-256-enc_layers-1-dec_layers-1-hid-256-dropout-0.3-bs-32-lr-0.001-tf-0.7-beam-5</strong> at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/b5rm47n2' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/b5rm47n2</a><br> View project at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a><br>Synced 5 W&B file(s), 11 media file(s), 22 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_092652-b5rm47n2/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mz3ojt1r with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'assignment_3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_094759-mz3ojt1r</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/mz3ojt1r' target=\"_blank\">prime-sweep-5</a></strong> to <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/mz3ojt1r' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/mz3ojt1r</a>"},"metadata":{}},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/15\nTrain Loss: 2.7495 | Train Token Acc: 17.73% | Train Seq Acc: 0.02%\nVal Loss:   2.5031 | Val Token Acc:   24.78% | Val Seq Acc:   0.15%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 0.15%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/15\nTrain Loss: 1.7820 | Train Token Acc: 43.63% | Train Seq Acc: 2.52%\nVal Loss:   1.5914 | Val Token Acc:   48.93% | Val Seq Acc:   8.33%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 8.33%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/15\nTrain Loss: 1.2509 | Train Token Acc: 58.86% | Train Seq Acc: 9.18%\nVal Loss:   1.3802 | Val Token Acc:   56.87% | Val Seq Acc:   16.61%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 16.61%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/15\nTrain Loss: 0.7112 | Train Token Acc: 76.60% | Train Seq Acc: 29.22%\nVal Loss:   1.2144 | Val Token Acc:   65.12% | Val Seq Acc:   26.91%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 26.91%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/15\nTrain Loss: 0.6582 | Train Token Acc: 78.38% | Train Seq Acc: 32.55%\nVal Loss:   1.2132 | Val Token Acc:   65.64% | Val Seq Acc:   28.26%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 28.26%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/15\nTrain Loss: 0.6083 | Train Token Acc: 80.03% | Train Seq Acc: 35.29%\nVal Loss:   1.2240 | Val Token Acc:   66.28% | Val Seq Acc:   29.15%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 29.15%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/15\nTrain Loss: 0.5687 | Train Token Acc: 81.38% | Train Seq Acc: 38.21%\nVal Loss:   1.1666 | Val Token Acc:   67.87% | Val Seq Acc:   31.45%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 31.45%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/15\nTrain Loss: 0.5336 | Train Token Acc: 82.50% | Train Seq Acc: 40.69%\nVal Loss:   1.2108 | Val Token Acc:   67.72% | Val Seq Acc:   30.70%\n------------------------------------------------------------\n‚ö†Ô∏è No improvement. Patience counter: 1/3\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/15\nTrain Loss: 0.5076 | Train Token Acc: 83.33% | Train Seq Acc: 42.28%\nVal Loss:   1.1726 | Val Token Acc:   68.78% | Val Seq Acc:   32.60%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 32.60%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/15\nTrain Loss: 0.4829 | Train Token Acc: 84.18% | Train Seq Acc: 44.39%\nVal Loss:   1.1512 | Val Token Acc:   68.82% | Val Seq Acc:   32.64%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 32.64%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/15\nTrain Loss: 0.4606 | Train Token Acc: 84.92% | Train Seq Acc: 46.12%\nVal Loss:   1.1677 | Val Token Acc:   69.14% | Val Seq Acc:   32.21%\n------------------------------------------------------------\n‚ö†Ô∏è No improvement. Patience counter: 1/3\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/15\nTrain Loss: 0.8689 | Train Token Acc: 70.86% | Train Seq Acc: 14.31%\nVal Loss:   1.7564 | Val Token Acc:   56.73% | Val Seq Acc:   18.22%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 18.22%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/15\nTrain Loss: 0.7378 | Train Token Acc: 75.18% | Train Seq Acc: 19.36%\nVal Loss:   1.6847 | Val Token Acc:   59.75% | Val Seq Acc:   21.50%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 21.50%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/15\nTrain Loss: 0.6467 | Train Token Acc: 78.35% | Train Seq Acc: 23.99%\nVal Loss:   1.6509 | Val Token Acc:   61.68% | Val Seq Acc:   24.57%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 24.57%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/15\nTrain Loss: 0.5752 | Train Token Acc: 80.84% | Train Seq Acc: 28.67%\nVal Loss:   1.6395 | Val Token Acc:   63.15% | Val Seq Acc:   26.60%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 26.60%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/15\nTrain Loss: 0.3703 | Train Token Acc: 87.78% | Train Seq Acc: 45.63%\nVal Loss:   1.6355 | Val Token Acc:   67.01% | Val Seq Acc:   31.58%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 31.58%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/15\nTrain Loss: 0.3467 | Train Token Acc: 88.64% | Train Seq Acc: 48.14%\nVal Loss:   1.6329 | Val Token Acc:   67.39% | Val Seq Acc:   31.84%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 31.84%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/15\nTrain Loss: 0.3223 | Train Token Acc: 89.44% | Train Seq Acc: 50.70%\nVal Loss:   1.6767 | Val Token Acc:   67.69% | Val Seq Acc:   32.58%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 32.58%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15/15\nTrain Loss: 0.3051 | Train Token Acc: 90.00% | Train Seq Acc: 52.69%\nVal Loss:   1.6714 | Val Token Acc:   67.75% | Val Seq Acc:   32.57%\n------------------------------------------------------------\n‚ö†Ô∏è No improvement. Patience counter: 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>train_loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train_sequence_accuracy</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train_token_accuracy</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_sequence_accuracy</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_token_accuracy</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_loss</td><td>0.30509</td></tr><tr><td>train_sequence_accuracy</td><td>0.52695</td></tr><tr><td>train_token_accuracy</td><td>0.89997</td></tr><tr><td>val_loss</td><td>1.67143</td></tr><tr><td>val_sequence_accuracy</td><td>0.32568</td></tr><tr><td>val_token_accuracy</td><td>0.67747</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">-cell-LSTMembed-128-enc_layers-3-dec_layers-2-hid-256-dropout-0.4-bs-32-lr-0.0001-tf-0.9-beam-1</strong> at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/w98fsu1b' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/w98fsu1b</a><br> View project at: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a><br>Synced 5 W&B file(s), 15 media file(s), 20 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_101245-w98fsu1b/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0d6pqdbq with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.9\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'assignment_3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_104818-0d6pqdbq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/0d6pqdbq' target=\"_blank\">cool-sweep-7</a></strong> to <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/sweeps/njnchcyt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/0d6pqdbq' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/assignment_3/runs/0d6pqdbq</a>"},"metadata":{}},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/15\nTrain Loss: 0.6009 | Train Token Acc: 80.01% | Train Seq Acc: 26.74%\nVal Loss:   1.6379 | Val Token Acc:   63.80% | Val Seq Acc:   28.10%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 28.10%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/15\nTrain Loss: 0.5407 | Train Token Acc: 82.05% | Train Seq Acc: 30.94%\nVal Loss:   1.6315 | Val Token Acc:   65.18% | Val Seq Acc:   29.64%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 29.64%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/15\nTrain Loss: 0.4884 | Train Token Acc: 83.87% | Train Seq Acc: 34.92%\nVal Loss:   1.6131 | Val Token Acc:   66.36% | Val Seq Acc:   31.30%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 31.30%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/15\nTrain Loss: 0.4503 | Train Token Acc: 85.15% | Train Seq Acc: 37.97%\nVal Loss:   1.6259 | Val Token Acc:   66.73% | Val Seq Acc:   32.51%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 32.51%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/15\nTrain Loss: 0.4145 | Train Token Acc: 86.31% | Train Seq Acc: 41.03%\nVal Loss:   1.6181 | Val Token Acc:   67.51% | Val Seq Acc:   33.24%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 33.24%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/15\nTrain Loss: 0.3192 | Train Token Acc: 89.51% | Train Seq Acc: 50.65%\nVal Loss:   1.5952 | Val Token Acc:   69.47% | Val Seq Acc:   35.82%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 35.82%\n","output_type":"stream"},{"name":"stderr","text":"Training:  33%|‚ñà‚ñà‚ñà‚ñé      | 989/2955 [00:46<01:31, 21.56it/s, loss=0.558, seq_acc=0.438, token_acc=0.83] ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\n\nclass TestEvaluator:\n    def __init__(self, model, test_loader, src_vocab, tgt_vocab, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.model = model.to(device)\n        self.test_loader = test_loader\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        self.device = device\n        self.inv_src_vocab = {v: k for k, v in src_vocab.items()}\n        self.inv_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n    def check_model_file(self, model_path):\n        \"\"\"Check if the model file exists and provide guidance if it doesn't.\"\"\"\n        if not os.path.exists(model_path):\n            error_msg = f\"Error: Model file not found at '{model_path}'.\\n\"\n            error_msg += \"Possible solutions:\\n\"\n            error_msg += \"1. Ensure training completed successfully and saved the model to '/kaggle/working/best_model.pt'.\\n\"\n            error_msg += \"2. Check if the model was saved to a different path and update 'model_path'.\\n\"\n            error_msg += \"3. Rerun the training script to generate the model.\\n\"\n            error_msg += \"4. If running in Kaggle, verify that '/kaggle/working/' is accessible and the file was persisted.\\n\"\n            error_msg += \"5. Provide the correct path to an existing model file.\"\n            raise FileNotFoundError(error_msg)\n        print(f\"Model file found at '{model_path}'.\")\n    def compute_sequence_accuracy(self, outputs, targets):\n        \"\"\"Compute sequence-level accuracy (exact match, ignoring special tokens).\"\"\"\n        outputs = outputs.argmax(dim=-1)  # [batch_size, seq_len]\n        correct = 0\n        total = outputs.size(0)\n        for pred, tgt in zip(outputs, targets):\n            pred = pred[(tgt != 0) & (tgt != 1) & (tgt != 2)]  # Exclude <PAD>, <SOS>, <EOS>\n            tgt = tgt[(tgt != 0) & (tgt != 1) & (tgt != 2)]\n            if torch.equal(pred, tgt):\n                correct += 1\n        return correct / total if total > 0 else 0.0\n\n    def evaluate_test_set(self):\n        \"\"\"Evaluate the model on the test set and return sequence accuracy.\"\"\"\n        self.model.eval()\n        total_seq_acc, total_samples = 0.0, 0\n\n        with torch.no_grad():\n            for src, tgt, _, _ in tqdm(self.test_loader, desc=\"Evaluating Test Set\"):\n                src, tgt = src.to(self.device), tgt.to(self.device)\n                output = self.model(src, tgt, teacher_forcing_ratio=0.0)\n                output = output[:, 1:].contiguous()  # Exclude <SOS>\n                seq_acc = self.compute_sequence_accuracy(output, tgt[:, 1:])\n                batch_size = src.size(0)\n                total_seq_acc += seq_acc * batch_size\n                total_samples += batch_size\n\n        avg_seq_acc = total_seq_acc / total_samples\n        return avg_seq_acc\n\n    def generate_predictions(self, output_dir=\"predictions_vanilla\", num_samples=10):\n        \"\"\"Generate predictions for the test set and save them to a file. Return samples for display.\"\"\"\n        self.model.eval()\n        predictions = []\n        sample_data = []\n\n        os.makedirs(output_dir, exist_ok=True)\n        pred_file = os.path.join(output_dir, \"predictions.tsv\")\n\n        with torch.no_grad():\n            for src, tgt, src_strs, tgt_strs in tqdm(self.test_loader, desc=\"Generating Predictions\"):\n                src = src.to(self.device)\n                preds = self.model.predict(src, max_len=30, beam_size=3)  # Use beam_size=3 as default\n                for i in range(len(preds)):\n                    input_str = src_strs[i]\n                    target_str = tgt_strs[i]\n                    pred_ids = preds[i]\n                    pred_str = ''.join([self.inv_tgt_vocab.get(id.item(), '?') for id in pred_ids if id.item() not in [0, self.tgt_vocab['<EOS>']]])\n                    predictions.append((input_str, target_str, pred_str))\n                    if len(sample_data) < num_samples:\n                        sample_data.append((input_str, target_str, pred_str))\n\n        # Save all predictions to a TSV file\n        pred_df = pd.DataFrame(predictions, columns=[\"Input\", \"Target\", \"Prediction\"])\n        pred_df.to_csv(pred_file, sep='\\t', index=False)\n\n        return sample_data, pred_file\n\n    def display_samples(self, sample_data):\n        \"\"\"Format sample predictions as a markdown table.\"\"\"\n        markdown = \"| Input | Target | Prediction | Match |\\n\"\n        markdown += \"|-------|--------|------------|-------|\\n\"\n        for input_str, target_str, pred_str in sample_data:\n            match = \"‚úÖ\" if pred_str == target_str else \"‚ùå\"\n            markdown += f\"| {input_str} | {target_str} | {pred_str} | {match} |\\n\"\n        return markdown\n\n    def display_samples_highlight_incorrect(self, sample_data):\n        \"\"\"Format sample predictions as a markdown table, highlighting incorrect predictions.\"\"\"\n        markdown = \"| Input | Target | Prediction | Match |\\n\"\n        markdown += \"|-------|--------|------------|-------|\\n\"\n        for input_str, target_str, pred_str in sample_data:\n            match = \"‚úÖ\" if pred_str == target_str else \"‚ùå\"\n            display_pred = f\"**{pred_str}**\" if pred_str != target_str else pred_str\n            markdown += f\"| {input_str} | {target_str} | {display_pred} | {match} |\\n\"\n        return markdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:44:50.324508Z","iopub.execute_input":"2025-05-18T04:44:50.324904Z","iopub.status.idle":"2025-05-18T04:44:50.347652Z","shell.execute_reply.started":"2025-05-18T04:44:50.324873Z","shell.execute_reply":"2025-05-18T04:44:50.347101Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nimport wandb\n\n# Assuming DataPreprocessor, Encoder, Decoder, Seq2Seq, and Trainer are defined as in the original code\n\ndef train_with_best_hyperparams(\n    train_path='/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv',\n    valid_path='/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv',\n    test_path='/kaggle/input/dakshina/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv',\n    device='cuda' if torch.cuda.is_available() else 'cpu',\n    save_path='/kaggle/working/best_model.pt'\n):\n    \"\"\"\n    Train the model with the best hyperparameters and save it to save_path.\n    Returns the trained model and vocabularies.\n    \"\"\"\n    # Best hyperparameters from WandB\n    config = {\n        'batch_size': 64,\n        'beam_size': 1,\n        'cell_type': 'GRU',\n        'dec_layers': 2,\n        'dropout': 0.2,\n        'emb_dim': 256,\n        'enc_layers': 3,\n        'epochs': 1,\n        'hidden_dim': 256,\n        'learning_rate': 0.001,\n        'patience': 3,\n        'teacher_forcing': 0.7\n    }\n\n    # Convert config to an object for compatibility with Trainer\n    class Config:\n        def __init__(self, params):\n            for key, value in params.items():\n                setattr(self, key, value)\n    \n    config_obj = Config(config)\n\n    # Initialize WandB run\n    wandb.init(project=\"transliteration\", config=config, name=\"best_hyperparams_run\")\n    \n    # Initialize DataPreprocessor\n    preprocessor = DataPreprocessor(batch_size=config['batch_size'], device=device)\n    \n    # Load datasets\n    train_data = preprocessor.load_dataset(train_path)\n    val_data = preprocessor.load_dataset(valid_path)\n    test_data = preprocessor.load_dataset(test_path)\n    \n    # Prepare data loaders\n    train_loader, val_loader, test_loader = preprocessor.prepare_data(train_data, val_data, test_data)\n    \n    # Initialize model\n    encoder = Encoder(\n        input_size=len(preprocessor.src_vocab),\n        embedding_dim=config['emb_dim'],\n        hidden_size=config['hidden_dim'],\n        num_layers=config['enc_layers'],\n        cell_type=config['cell_type'],\n        dropout=config['dropout']\n    )\n    decoder = Decoder(\n        output_size=len(preprocessor.tgt_vocab),\n        embedding_dim=config['emb_dim'],\n        hidden_size=config['hidden_dim'],\n        num_layers=config['dec_layers'],\n        cell_type=config['cell_type'],\n        dropout=config['dropout']\n    )\n    model = Seq2Seq(encoder, decoder).to(device)\n    \n    # Initialize Trainer\n    trainer = Trainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        config=config_obj,\n        device=device,\n        save_path=save_path\n    )\n    \n    # Train the model\n    trainer.train(preprocessor.src_vocab, preprocessor.tgt_vocab)\n    \n    # Finish WandB run\n    wandb.finish()\n    \n    print(f\"Training completed. Model saved to {save_path}\")\n    return model, preprocessor.src_vocab, preprocessor.tgt_vocab, test_loader\n\ndef evaluate_with_best_model(\n    model,\n    test_loader,\n    src_vocab,\n    tgt_vocab,\n    model_path='/kaggle/working/best_model.pt',\n    device='cuda' if torch.cuda.is_available() else 'cpu'\n):\n    \"\"\"\n    Evaluate the model on the test set and generate predictions with highlighted incorrect outputs.\n    Returns sequence accuracy, markdown table, and predictions file path.\n    \"\"\"\n    # Initialize TestEvaluator\n    evaluator = TestEvaluator(model, test_loader, src_vocab, tgt_vocab, device)\n    \n    # Check model file\n    evaluator.check_model_file(model_path)\n    \n    # Load the model (already loaded in model, but verify for consistency)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    print(f\"Loaded model from {model_path}\")\n    \n    # Evaluate on test set\n    test_seq_acc = evaluator.evaluate_test_set()\n    print(f\"\\nTest Set Sequence Accuracy: {test_seq_acc*100:.2f}%\")\n    \n    # Generate predictions\n    sample_data, pred_file = evaluator.generate_predictions(num_samples=10)\n    print(f\"\\nPredictions saved to {pred_file}\")\n    print(pred_file.sample(20))\n    # Display sample predictions with incorrect ones highlighted\n    markdown_table = evaluator.display_samples_highlight_incorrect(sample_data)\n    print(\"\\nSample Predictions (Incorrect Predictions Highlighted):\")\n    print(markdown_table)\n    \n    return test_seq_acc, markdown_table, pred_file\n\nif __name__ == \"__main__\":\n    try:\n        # Train with best hyperparameters\n        model, src_vocab, tgt_vocab, test_loader = train_with_best_hyperparams()\n        \n        # Evaluate on test set\n        test_seq_acc, markdown_table, pred_file = evaluate_with_best_model(\n            model, test_loader, src_vocab, tgt_vocab\n        )\n    except FileNotFoundError as e:\n        print(e)\n    except Exception as e:\n        print(f\"Error during training or evaluation: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:51:27.732641Z","iopub.execute_input":"2025-05-18T04:51:27.732920Z","iopub.status.idle":"2025-05-18T04:55:00.129196Z","shell.execute_reply.started":"2025-05-18T04:51:27.732898Z","shell.execute_reply":"2025-05-18T04:55:00.128510Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_045127-ho5az2n0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bgorai005-iit-madras/transliteration/runs/ho5az2n0' target=\"_blank\">best_hyperparams_run</a></strong> to <a href='https://wandb.ai/bgorai005-iit-madras/transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bgorai005-iit-madras/transliteration' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bgorai005-iit-madras/transliteration/runs/ho5az2n0' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/transliteration/runs/ho5az2n0</a>"},"metadata":{}},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/1\nTrain Loss: 1.2208 | Train Token Acc: 61.94% | Train Seq Acc: 14.63%\nVal Loss:   1.5134 | Val Token Acc:   60.00% | Val Seq Acc:   22.58%\n------------------------------------------------------------\n‚úÖ New best model saved with val sequence accuracy: 22.58%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ</td></tr><tr><td>train_loss</td><td>‚ñÅ</td></tr><tr><td>train_sequence_accuracy</td><td>‚ñÅ</td></tr><tr><td>train_token_accuracy</td><td>‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ</td></tr><tr><td>val_sequence_accuracy</td><td>‚ñÅ</td></tr><tr><td>val_token_accuracy</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_loss</td><td>1.22085</td></tr><tr><td>train_sequence_accuracy</td><td>0.14631</td></tr><tr><td>train_token_accuracy</td><td>0.61935</td></tr><tr><td>val_loss</td><td>1.51341</td></tr><tr><td>val_sequence_accuracy</td><td>0.22578</td></tr><tr><td>val_token_accuracy</td><td>0.60005</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">best_hyperparams_run</strong> at: <a href='https://wandb.ai/bgorai005-iit-madras/transliteration/runs/ho5az2n0' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/transliteration/runs/ho5az2n0</a><br> View project at: <a href='https://wandb.ai/bgorai005-iit-madras/transliteration' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/transliteration</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250518_045127-ho5az2n0/logs</code>"},"metadata":{}},{"name":"stdout","text":"Training completed. Model saved to /kaggle/working/best_model.pt\nModel file found at '/kaggle/working/best_model.pt'.\nLoaded model from /kaggle/working/best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Test Set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 145/145 [00:04<00:00, 35.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTest Set Sequence Accuracy: 23.33%\n","output_type":"stream"},{"name":"stderr","text":"Generating Predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 145/145 [01:52<00:00,  1.28it/s]","output_type":"stream"},{"name":"stdout","text":"\nPredictions saved to predictions_vanilla/predictions.tsv\nError during training or evaluation: 'str' object has no attribute 'sample'\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\n\n# Correctly load the TSV file into a DataFrame\ndf = pd.read_csv('/kaggle/working/predictions_vanilla/predictions.tsv', sep='\\t')\n\n# Display the first few rows\nprint(df.sample(200))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:58:21.711700Z","iopub.execute_input":"2025-05-18T04:58:21.711993Z","iopub.status.idle":"2025-05-18T04:58:21.737042Z","shell.execute_reply.started":"2025-05-18T04:58:21.711961Z","shell.execute_reply":"2025-05-18T04:58:21.736483Z"}},"outputs":[{"name":"stdout","text":"                  Input         Target   Prediction\n6584        mailashtona      ‡¶Æ‡¶æ‡¶á‡¶≤‡¶∏‡ßç‡¶ü‡ßã‡¶®    ‡¶Æ‡¶æ‡¶á‡¶≤‡¶∏‡ßç‡¶§‡¶æ‡¶®\n4524                not            ‡¶®‡ßã‡¶ü           ‡¶®‡¶ü\n7348             rurala          ‡¶∞‡ßÅ‡¶∞‡¶æ‡¶≤         ‡¶∞‡ßÅ‡¶≤‡¶æ\n8792              stari         ‡¶∏‡ßç‡¶ü‡ßã‡¶∞‡¶ø      ‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶æ‡¶á\n7836          sheleshma       ‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡ßç‡¶Æ‡¶æ       ‡¶∏‡ßá‡¶≤‡ßá‡¶∏‡¶Æ\n...                 ...            ...          ...\n6686              marbo          ‡¶Æ‡¶æ‡¶∞‡¶¨‡ßã        ‡¶Æ‡¶æ‡¶∞‡ßç‡¶¨\n8146              sarju           ‡¶∏‡¶∞‡¶Ø‡ßÇ       ‡¶∏‡¶æ‡¶∞‡ßç‡¶ú‡ßÅ\n2267              ginir          ‡¶ó‡¶ø‡¶®‡¶ø‡¶∞        ‡¶ú‡¶ø‡¶®‡¶ø‡¶∞\n2441        grambaseeke    ‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ‡¶¨‡¶æ‡¶∏‡ßÄ‡¶ï‡ßá  ‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ‡¶¨‡¶ø‡¶ï‡ßá‡¶∑‡ßá\n2589  chalachchitrogulo  ‡¶ö‡¶≤‡¶ö‡ßç‡¶ö‡¶ø‡¶§‡ßç‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã  ‡¶ö‡¶≤‡¶ö‡¶ø‡¶§‡ßç‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã\n\n[200 rows x 3 columns]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}